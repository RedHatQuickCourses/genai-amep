= Deployment Overview

This section provides an overview of the GenAI AMEP deployment architecture on OpenShift AI, covering the complete workflow from model packaging to evaluation.

== Architecture Components

The GenAI AMEP deployment consists of several integrated components:

=== Core Infrastructure

* *Quay Registry*: Enterprise container registry for storing ModelCar OCI images
* *Model Registry*: MySQL-backed registry for tracking model metadata and versions
* *MinIO Object Storage*: S3-compatible storage for evaluation results and artifacts
* *Tekton Pipelines*: Kubernetes-native CI/CD for automating workflows

=== Pipeline Components

==== ModelCar Pipeline
The ModelCar pipeline automates the process of packaging AI models as OCI (Open Container Initiative) images:

* *Hugging Face Model Download*: Pulls models from Hugging Face Hub
* *Model Compression* (Optional): Compresses models using llmcompressor
* *ModelCar Building*: Packages models into OCI images using OLOT
* *Registry Integration*: Pushes images to Quay and registers in Model Registry
* *Model Deployment* (Optional): Deploys as vLLM InferenceService
* *Model Evaluation* (Optional): Runs evaluation tasks

==== Evaluation Pipeline
The evaluation pipeline provides comprehensive model assessment:

* *GuideLLM Benchmarking*: Performance and throughput testing
* *LM-Eval Tasks*: Accuracy evaluation (MMLU, HumanEval, MBPP, etc.)
* *Multi-language Evaluation*: Cross-language testing capabilities
* *TrustyAI Integration*: Bias and fairness assessment
* *Results Storage*: Automatic upload to S3-compatible storage

== Deployment Workflow

The typical deployment workflow follows these steps:

[source,mermaid]
----
graph LR
    A[Download Model] --> B[Optional: Compress]
    B --> C[Build ModelCar]
    C --> D[Push to Quay]
    D --> E[Register in Model Registry]
    E --> F[Optional: Deploy]
    F --> G[Optional: Evaluate]
    G --> H[Store Results]
----

. *Infrastructure Setup*: Deploy Quay, Model Registry, and MinIO
. *Pipeline Installation*: Install Tekton tasks and pipelines
. *Model Packaging*: Run ModelCar pipeline to package models
. *Model Deployment*: Deploy models as InferenceServices
. *Model Evaluation*: Run evaluation pipelines to assess performance
. *Results Analysis*: Review evaluation results from S3 storage

== Key Features

=== ModelCar Pipeline Features

* Automated download from Hugging Face Hub
* Support for gated models with authentication
* Optional model compression for reduced size
* OCI image packaging using OLOT
* Automatic model registry integration
* Configurable task skipping for flexibility
* Support for vLLM deployment
* Integrated evaluation capabilities

=== Evaluation Pipeline Features

* Performance benchmarking with GuideLLM
* Accuracy evaluation with lm-eval-harness
* Custom evaluation task support
* Multi-language evaluation
* Bias and fairness testing with TrustyAI
* Automated results storage
* Model registry integration

== Prerequisites

Before deploying the GenAI AMEP components on OpenShift AI, ensure you have:

* OpenShift AI or OpenShift cluster with admin access
* OpenShift Pipelines (Tekton) operator installed
* Sufficient storage for model files and evaluation results
* Access to Quay.io or internal registry for container images
* Hugging Face token (for gated models)

== Next Steps

Follow the deployment guides in this chapter:

. xref:infrastructure-setup.adoc[Infrastructure Setup] - Deploy Quay, Model Registry, and MinIO
. xref:modelcar-pipeline.adoc[ModelCar Pipeline Deployment] - Set up model packaging automation
. xref:evaluation-pipeline.adoc[Evaluation Pipeline Deployment] - Deploy model evaluation workflows
