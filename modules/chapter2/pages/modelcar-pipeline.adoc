= ModelCar Pipeline Deployment

This guide covers the deployment and usage of the ModelCar pipeline, which automates the packaging of AI models as OCI images on OpenShift AI.

== Overview

The ModelCar pipeline automates the complete workflow of downloading, optionally compressing, packaging, and deploying AI models. It uses Tekton to orchestrate the following tasks:

. Clean up workspace
. Download model from Hugging Face
. Compress model (optional)
. Build ModelCar OCI image using OLOT
. Push image to Quay registry
. Register model in Model Registry
. Deploy model as InferenceService (optional)
. Evaluate model (optional)

== Prerequisites

Before deploying the ModelCar pipeline, ensure you have completed:

* xref:infrastructure-setup.adoc[Infrastructure Setup] - Quay, Model Registry, and MinIO deployed
* Pipeline namespace created (e.g., `modelcar-pipelines`)
* Secrets configured:
** `quay-auth-secret` - Quay robot account credentials
** `huggingface-secret` - Hugging Face token (for gated models)
** `s3-credentials` - MinIO/S3 credentials

== Deploy Pipeline Tasks

The ModelCar pipeline consists of several reusable Tekton tasks.

=== Create Pipeline Storage

Create a PersistentVolumeClaim for pipeline workspace:

[source,bash]
----
oc apply -f deploy/openshift/modelcar-storage.yaml -n modelcar-pipelines
----

This creates a PVC named `modelcar-workspace-pvc` for storing model files during pipeline execution.

=== Deploy Compression Task

The compression task uses llmcompressor to reduce model size:

[source,bash]
----
oc apply -f deploy/openshift/modelcar-compress-task.yaml -n modelcar-pipelines
----

=== Deploy Evaluation Tasks

Deploy the evaluation tasks for model assessment:

[source,bash]
----
# Standard evaluation task
oc apply -f deploy/openshift/modelcar-evaluate-task.yaml -n modelcar-pipelines

# GuideLLM performance evaluation
oc apply -f deploy/openshift/modelcar-guidellm-containerized-task.yaml -n modelcar-pipelines

# Multi-language evaluation
oc apply -f deploy/openshift/modelcar-multilang-evaluate-task.yaml -n modelcar-pipelines

# TrustyAI bias/fairness evaluation
oc apply -f deploy/openshift/modelcar-trustyai-task.yaml -n modelcar-pipelines
----

=== Deploy Main Pipeline

Deploy the main ModelCar pipeline:

[source,bash]
----
oc apply -f deploy/openshift/modelcar-pipeline.yaml -n modelcar-pipelines
----

=== Verify Pipeline Deployment

Check that all tasks and the pipeline are created:

[source,bash]
----
oc get tasks -n modelcar-pipelines
oc get pipelines -n modelcar-pipelines
----

== Running the ModelCar Pipeline

=== Basic Pipeline Run

To package a model from Hugging Face:

[source,bash]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: modelcar-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: modelcar-pipeline
  params:
    - name: HUGGINGFACE_MODEL
      value: "ibm-granite/granite-3.3-2b-instruct"
    - name: OCI_IMAGE
      value: "quay.io/<YOUR_ORG>/granite-3.3-2b-instruct"
    - name: MODEL_NAME
      value: "granite-3.3-2b-instruct"
    - name: MODEL_VERSION
      value: "1.0.0"
    - name: MODEL_REGISTRY_URL
      value: "http://mysql.rhoai-model-registries.svc.cluster.local:3306"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: modelcar-workspace-pvc
    - name: quay-auth-workspace
      secret:
        secretName: quay-auth-secret
EOF
----

=== Pipeline with Model Compression

To compress the model before packaging:

[source,bash]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: modelcar-compress-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: modelcar-pipeline
  params:
    - name: HUGGINGFACE_MODEL
      value: "ibm-granite/granite-3.3-8b-instruct"
    - name: OCI_IMAGE
      value: "quay.io/<YOUR_ORG>/granite-3.3-8b-instruct-compressed"
    - name: MODEL_NAME
      value: "granite-3.3-8b-instruct-compressed"
    - name: MODEL_VERSION
      value: "1.0.0"
    - name: COMPRESS_MODEL
      value: "true"
    - name: MODEL_REGISTRY_URL
      value: "http://mysql.rhoai-model-registries.svc.cluster.local:3306"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: modelcar-workspace-pvc
    - name: quay-auth-workspace
      secret:
        secretName: quay-auth-secret
EOF
----

=== Pipeline with Deployment and Evaluation

To package, deploy, and evaluate a model:

[source,bash]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: modelcar-full-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: modelcar-pipeline
  params:
    - name: HUGGINGFACE_MODEL
      value: "ibm-granite/granite-3.3-2b-instruct"
    - name: OCI_IMAGE
      value: "quay.io/<YOUR_ORG>/granite-3.3-2b-instruct"
    - name: MODEL_NAME
      value: "granite-3.3-2b-instruct"
    - name: MODEL_VERSION
      value: "1.0.0"
    - name: MODEL_REGISTRY_URL
      value: "http://mysql.rhoai-model-registries.svc.cluster.local:3306"
    - name: DEPLOY_MODEL
      value: "true"
    - name: EVALUATE_MODEL
      value: "true"
    - name: TASKS
      value: "arc_easy,hellaswag,winogrande,humaneval,mbpp"
    - name: GUIDELLM_EVALUATE_MODEL
      value: "true"
    - name: S3_ENABLED
      value: "true"
    - name: MAX_MODEL_LEN
      value: "8192"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: modelcar-workspace-pvc
    - name: quay-auth-workspace
      secret:
        secretName: quay-auth-secret
EOF
----

=== Pipeline with Gated Model

For gated models requiring authentication:

[source,bash]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: modelcar-gated-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: modelcar-pipeline
  params:
    - name: HUGGINGFACE_MODEL
      value: "meta-llama/Llama-3.2-3B-Instruct"
    - name: OCI_IMAGE
      value: "quay.io/<YOUR_ORG>/llama-3.2-3b-instruct"
    - name: MODEL_NAME
      value: "llama-3.2-3b-instruct"
    - name: MODEL_VERSION
      value: "1.0.0"
    - name: MODEL_REGISTRY_URL
      value: "http://mysql.rhoai-model-registries.svc.cluster.local:3306"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: modelcar-workspace-pvc
    - name: quay-auth-workspace
      secret:
        secretName: quay-auth-secret
EOF
----

NOTE: Ensure the `huggingface-secret` contains a valid token with access to the gated model.

== Pipeline Parameters Reference

The ModelCar pipeline supports the following parameters:

[cols="1,2,1,2"]
|===
|Parameter |Description |Required |Default

|`HUGGINGFACE_MODEL`
|Hugging Face model repository (e.g., `ibm-granite/granite-3.3-2b-instruct`)
|Yes
|N/A

|`OCI_IMAGE`
|Destination OCI image (e.g., `quay.io/org/model:tag`)
|Yes
|N/A

|`HUGGINGFACE_ALLOW_PATTERNS`
|File patterns to download (e.g., `"*.safetensors", "*.json"`)
|No
|`*.safetensors, *.json, *.txt`

|`COMPRESS_MODEL`
|Whether to compress the model (`true`/`false`)
|No
|`false`

|`MODEL_NAME`
|Name to register in model registry
|Yes
|N/A

|`MODEL_VERSION`
|Version to register in model registry
|No
|`1.0.0`

|`MODEL_REGISTRY_URL`
|URL of the model registry service
|Yes
|N/A

|`DEPLOY_MODEL`
|Whether to deploy as InferenceService (`true`/`false`)
|No
|`false`

|`EVALUATE_MODEL`
|Whether to run evaluation (`true`/`false`)
|No
|`false`

|`TASKS`
|Comma-separated list of evaluation tasks
|No
|`humaneval,mbpp`

|`GUIDELLM_EVALUATE_MODEL`
|Whether to run GuideLLM evaluation (`true`/`false`)
|No
|`false`

|`MULTILANG_EVALUATE_MODEL`
|Whether to run multi-language evaluation (`true`/`false`)
|No
|`false`

|`TRUSTYAI_EVALUATE_MODEL`
|Whether to run TrustyAI evaluation (`true`/`false`)
|No
|`false`

|`S3_ENABLED`
|Whether to upload results to S3 (`true`/`false`)
|No
|`false`

|`SKIP_TASKS`
|Comma-separated list of tasks to skip
|No
|``

|`MAX_MODEL_LEN`
|Maximum model length for vLLM
|No
|`8192`

|`VLLM_ARGS`
|Additional vLLM arguments
|No
|``
|===

== Monitoring Pipeline Execution

=== View Pipeline Run Status

[source,bash]
----
# List all pipeline runs
oc get pipelinerun -n modelcar-pipelines

# Watch a specific pipeline run
oc get pipelinerun <PIPELINERUN_NAME> -n modelcar-pipelines -w

# Describe pipeline run for details
oc describe pipelinerun <PIPELINERUN_NAME> -n modelcar-pipelines
----

=== View Task Logs

[source,bash]
----
# List task runs
oc get taskrun -n modelcar-pipelines

# View logs for a specific task
oc logs -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -l tekton.dev/pipelineTask=<TASK_NAME> \
  -n modelcar-pipelines

# Examples:
# View download task logs
oc logs -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -l tekton.dev/pipelineTask=pull-model-from-huggingface \
  -n modelcar-pipelines

# View compression task logs
oc logs -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -l tekton.dev/pipelineTask=compress-model \
  -n modelcar-pipelines
----

=== Using OpenShift Console

. Navigate to Pipelines â†’ PipelineRuns in the OpenShift Console
. Select your namespace (`modelcar-pipelines`)
. Click on a pipeline run to view details
. View logs for each task in the visual pipeline graph

== Container Images

The pipeline uses several custom container images that must be built and pushed to your registry:

=== Hugging Face ModelCar Builder

Location: `deploy/tasks/huggingface-modelcar-builder/`

[source,bash]
----
cd deploy/tasks/huggingface-modelcar-builder
podman build --platform linux/amd64 \
  -t quay.io/<YOUR_ORG>/huggingface-modelcar-builder:latest \
  -f Containerfile .
podman push quay.io/<YOUR_ORG>/huggingface-modelcar-builder:latest
----

=== ModelCar Builder

Location: `deploy/tasks/build-and-push-modelcar/`

[source,bash]
----
cd deploy/tasks/build-and-push-modelcar
podman build --platform linux/amd64 \
  -t quay.io/<YOUR_ORG>/modelcar-builder:latest \
  -f Containerfile .
podman push quay.io/<YOUR_ORG>/modelcar-builder:latest
----

=== ModelCar Registry Integration

Location: `deploy/tasks/register-with-registry/`

[source,bash]
----
cd deploy/tasks/register-with-registry
podman build --platform linux/amd64 \
  -t quay.io/<YOUR_ORG>/modelcar-register:latest \
  -f Containerfile .
podman push quay.io/<YOUR_ORG>/modelcar-register:latest
----

NOTE: Update the image references in the pipeline YAML files to point to your custom images.

== Troubleshooting

=== Pipeline Run Fails at Download Step

Check Hugging Face token and model access:

[source,bash]
----
# Verify secret exists
oc get secret huggingface-secret -n modelcar-pipelines

# Check if model is gated
# Visit https://huggingface.co/<MODEL_REPO> and verify access
----

=== Pipeline Run Fails at Push Step

Check Quay authentication:

[source,bash]
----
# Verify Quay secret exists
oc get secret quay-auth-secret -n modelcar-pipelines

# Test Quay login
podman login <QUAY_ROUTE> --authfile=/path/to/auth.json
----

=== Model Registry Connection Issues

Verify Model Registry connectivity:

[source,bash]
----
# Check MySQL service
oc get svc mysql -n rhoai-model-registries

# Test connection from pipeline namespace
oc run test-mysql --rm -it --image=mysql:8.0 -n modelcar-pipelines -- \
  mysql -h mysql.rhoai-model-registries.svc.cluster.local \
  -u <USERNAME> -p<PASSWORD> -e "SHOW DATABASES;"
----

=== Out of Memory Errors

Increase memory limits in pipeline task specs or use compression:

[source,bash]
----
# Edit pipeline to increase memory
oc edit pipeline modelcar-pipeline -n modelcar-pipelines

# Or set COMPRESS_MODEL=true to reduce memory usage
----

== Next Steps

* xref:evaluation-pipeline.adoc[Deploy Evaluation Pipeline] - Set up model evaluation workflows
* Learn about customizing evaluation tasks
* Integrate with CI/CD workflows
