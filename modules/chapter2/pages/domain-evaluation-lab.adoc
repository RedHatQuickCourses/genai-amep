= Lab: Domain-Specific Evaluation
:navtitle: Domain-Specific Evaluation
:toc: macro

// Antora metadata
:page-role: MLOps Engineer
:description: Create and run custom evaluation tasks tailored to your business domain.

[.lead]
*Standard benchmarks test general intelligence. Domain evaluations test business value.*

While standardized benchmarks (MMLU, ARC, HellaSwag) measure general model capabilities, they don't tell you if your model understands *your* business domain. This lab teaches you to create and run domain-specific evaluations that measure model performance on tasks relevant to your use case.

== Why Domain-Specific Evaluation Matters

Domain-specific evaluation answers critical business questions:

* **Does the model understand our terminology?** (e.g., medical jargon, legal terms, financial concepts)
* **Can it handle our specific use cases?** (e.g., insurance claim processing, code review, customer support)
* **Is it safe for our domain?** (e.g., healthcare compliance, financial regulations)
* **Does fine-tuning improve domain performance?** (measuring ROI of customization efforts)

== Prerequisites

Before starting, ensure:

* Your InferenceService is deployed and in "Ready" state
* TrustyAI operator is installed and configured
* You have domain-specific questions/prompts ready
* You understand your model's expected behavior in your domain

== Step 1: Prepare Domain-Specific Dataset

First, we'll create a custom evaluation dataset with domain-specific questions:

[source,bash]
----
# Create evaluation directory
mkdir -p ~/domain-evaluation
cd ~/domain-evaluation

# Create a custom evaluation dataset
# Format: JSONL (one JSON object per line)
cat > domain-dataset.jsonl <<EOF
{"input": "What is the recommended treatment for Type 2 diabetes?", "expected_output": "Metformin is typically the first-line treatment", "domain": "healthcare"}
{"input": "Explain the difference between a deductible and a copay.", "expected_output": "A deductible is the amount you pay before insurance kicks in, while a copay is a fixed amount per service", "domain": "insurance"}
{"input": "What are the key requirements for GDPR compliance?", "expected_output": "GDPR requires data protection, user consent, right to deletion, and breach notification", "domain": "legal"}
{"input": "Calculate the compound interest for $10,000 at 5% APR over 10 years.", "expected_output": "Approximately $16,288.95", "domain": "finance"}
EOF

echo "✅ Created domain dataset with $(wc -l < domain-dataset.jsonl) examples"
----

[NOTE]
.Dataset Format
====
The dataset uses JSONL format (one JSON object per line). Each object should have:
* `input`: The question or prompt
* `expected_output`: The expected answer (for reference)
* `domain`: Your business domain (optional, for categorization)

You can add more fields as needed for your evaluation.
====

== Step 2: Create Custom Evaluation Task Definition

Create a TrustyAI-compatible task definition:

[source,bash]
----
# Create task definition
cat > domain-task.yaml <<EOF
task: domain_evaluation
group: custom
description: "Domain-specific evaluation for business use cases"
metrics:
  - accuracy
  - exact_match
output_type: generate_until
EOF

echo "✅ Created task definition"
----

== Step 3: Upload Dataset to S3

Upload your dataset to S3 so TrustyAI can access it:

[source,bash]
----
# Set S3 configuration
export S3_ENDPOINT="http://minio-service.s3-storage.svc.cluster.local:9000"
export S3_BUCKET="model-evaluations"
export S3_ACCESS_KEY="minio"
export S3_SECRET_KEY="minio123"

# Upload dataset
oc run aws-cli-upload --rm -i --restart=Never \
  --image=amazon/aws-cli:latest \
  --env="AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}" \
  --env="AWS_SECRET_ACCESS_KEY=${S3_SECRET_KEY}" \
  --env="AWS_DEFAULT_REGION=us-east-1" \
  -- sh -c "
    aws --endpoint-url=${S3_ENDPOINT} s3 mb s3://${S3_BUCKET} || true
    
    aws --endpoint-url=${S3_ENDPOINT} s3 cp \
      ~/domain-evaluation/domain-dataset.jsonl \
      s3://${S3_BUCKET}/custom-data/domain-dataset.jsonl
    
    aws --endpoint-url=${S3_ENDPOINT} s3 cp \
      ~/domain-evaluation/domain-task.yaml \
      s3://${S3_BUCKET}/custom-data/domain-task.yaml
    
    echo \"✅ Dataset uploaded to S3\"
  "
----

== Step 4: Create LMEvalJob with Custom Task

Create an LMEvalJob that uses your custom dataset:

[source,bash]
----
# Set your model information
export NAMESPACE="vllm"
export MODEL_NAME="granite-4.0-micro"
export INFERENCE_URL=$(oc get inferenceservice $MODEL_NAME -n ${NAMESPACE} \
  -o jsonpath='{.status.url}')

# Create custom evaluation job
cat > domain-eval-job.yaml <<EOF
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: domain-specific-eval
  namespace: ${NAMESPACE}
spec:
  model: local-completions
  taskList:
    taskNames:
      - domain_evaluation
  modelArgs:
    - name: model
      value: ${MODEL_NAME}
    - name: base_url
      value: ${INFERENCE_URL}/v1/completions
    - name: num_concurrent
      value: '1'
    - name: max_retries
      value: '6'
    - name: tokenized_requests
      value: 'False'
    - name: tokenizer
      value: ibm-granite/granite-4.0-micro
  logSamples: true
  batchSize: '1'
  allowOnline: true
  allowCodeExecution: false
  outputs:
    pvcManaged:
      size: 5Gi
  pod:
    volumes:
      - name: custom-dataset
        configMap:
          name: domain-dataset-cm
    container:
      volumeMounts:
        - name: custom-dataset
          mountPath: /opt/app-root/src/custom-tasks
          readOnly: true
EOF

# Create ConfigMap with custom dataset
oc create configmap domain-dataset-cm \
  --from-file=domain-dataset.jsonl=~/domain-evaluation/domain-dataset.jsonl \
  --from-file=domain-task.yaml=~/domain-evaluation/domain-task.yaml \
  -n ${NAMESPACE}

# Apply the evaluation job
oc apply -f domain-eval-job.yaml
----

[NOTE]
.Custom Task Configuration
====
For more complex custom tasks, you may need to:
* Define custom metrics in the task YAML
* Configure task-specific parameters
* Handle different output formats

Refer to the TrustyAI documentation for advanced custom task configuration.
====

== Step 5: Monitor Evaluation Progress

Watch the evaluation job:

[source,bash]
----
# Watch job status
oc get lmevaljob domain-specific-eval -n ${NAMESPACE} -w
----

Monitor the pod logs:

[source,bash]
----
# Get pod name
POD_NAME=$(oc get pods -n ${NAMESPACE} -l job-name=domain-specific-eval \
  -o jsonpath='{.items[0].metadata.name}')

# Watch logs
oc logs -f $POD_NAME -n ${NAMESPACE}
----

== Step 6: Extract and Analyze Results

Once the evaluation completes, extract the results:

[source,bash]
----
# Extract results (similar to multi-language evaluation)
# Get the PVC name
PVC_NAME=$(oc get lmevaljob domain-specific-eval -n ${NAMESPACE} \
  -o jsonpath='{.status.outputs.pvcManaged.name}')

# Create extraction pod
cat > extract-domain-results.yaml <<EOF
apiVersion: v1
kind: Pod
metadata:
  name: extract-domain-results
  namespace: ${NAMESPACE}
spec:
  containers:
  - name: extractor
    image: registry.access.redhat.com/ubi8/ubi-minimal:latest
    command: ["/bin/sh", "-c", "sleep 3600"]
    volumeMounts:
    - name: results
      mountPath: /results
  volumes:
  - name: results
    persistentVolumeClaim:
      claimName: ${PVC_NAME}
  restartPolicy: Never
EOF

oc apply -f extract-domain-results.yaml
oc wait --for=condition=Ready pod/extract-domain-results -n ${NAMESPACE} --timeout=60s

# Copy results
mkdir -p ~/evaluation-results/domain
oc cp ${NAMESPACE}/extract-domain-results:/results ~/evaluation-results/domain/

# Cleanup
oc delete pod extract-domain-results -n ${NAMESPACE}
rm extract-domain-results.yaml

echo "✅ Results extracted to ~/evaluation-results/domain/"
----

== Step 7: Upload Results to S3

Upload results for later comparison:

[source,bash]
----
# Upload to S3
oc run aws-cli-upload --rm -i --restart=Never \
  --image=amazon/aws-cli:latest \
  --env="AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}" \
  --env="AWS_SECRET_ACCESS_KEY=${S3_SECRET_KEY}" \
  --env="AWS_DEFAULT_REGION=us-east-1" \
  -- sh -c "
    TIMESTAMP=\$(date +%Y%m%d_%H%M%S)
    MODEL_ID=\"${MODEL_NAME}_v1.0\"
    
    aws --endpoint-url=${S3_ENDPOINT} s3 sync \
      ~/evaluation-results/domain/ \
      s3://${S3_BUCKET}/domain/\${MODEL_ID}/\${TIMESTAMP}/
    
    echo \"✅ Domain evaluation results uploaded\"
  "
----

== Understanding Domain-Specific Results

Domain evaluation results typically include:

* **Accuracy:** Percentage of correct answers
* **Exact Match:** Percentage of responses that exactly match expected output
* **Domain-Specific Metrics:** Custom metrics relevant to your use case
* **Sample Responses:** Actual model outputs for manual review

=== Interpreting Results

* **High Accuracy (>80%):** Model understands your domain well
* **Medium Accuracy (60-80%):** Model has gaps but may be usable with prompt engineering
* **Low Accuracy (<60%):** Model may need fine-tuning or domain-specific training

[NOTE]
.Domain Evaluation Best Practices
====
* Start with a small, representative dataset (10-20 examples)
* Include edge cases and common failure modes
* Test both simple and complex queries
* Compare results before and after fine-tuning
* Regularly update your dataset as your domain evolves
====

== Example: Healthcare Domain Evaluation

Here's an example for healthcare domain:

[source,bash]
----
# Healthcare-specific dataset
cat > healthcare-dataset.jsonl <<EOF
{"input": "What are the symptoms of Type 2 diabetes?", "expected_output": "Increased thirst, frequent urination, fatigue, blurred vision", "domain": "healthcare", "category": "symptoms"}
{"input": "What is the normal blood pressure range?", "expected_output": "Normal blood pressure is typically below 120/80 mmHg", "domain": "healthcare", "category": "vitals"}
{"input": "Explain the mechanism of action of metformin.", "expected_output": "Metformin reduces glucose production in the liver and improves insulin sensitivity", "domain": "healthcare", "category": "medications"}
EOF
----

== Next Steps

* Proceed to xref:results-collection.adoc[Results Collection & Visualization] to analyze all your evaluation results together
* Compare domain-specific results with general benchmarks to identify gaps
* Use results to inform fine-tuning or prompt engineering efforts

---
*Domain knowledge tested. Now let's visualize all the results.*
