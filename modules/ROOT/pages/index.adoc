= Automated Multi-Language LLM Evaluation Pipeline
:navtitle: Introduction

== Overview

This guide documents an automated pipeline for evaluating Large Language Models (LLMs) across multiple languages using Red Hat OpenShift AI as the platform.

*Developed by*: Red Hat AI Customer Adoption and Innovation team (CAI)

== What This Pipeline Does

This automated solution provides end-to-end capabilities for:

. *Model Acquisition*: Download models from HuggingFace Hub
. *Model Optimization*: Optional compression using llmcompressor
. *Container Packaging*: Package models in Modelcar OCI containers
. *Registry Management*: Register models in OpenShift AI Model Registry
. *Model Deployment*: Deploy models using vLLM runtime for inference
. *Multi-Language Evaluation*: Test model performance across English, Spanish, and Japanese
. *Bias Detection*: Optional TrustyAI integration for fairness testing
. *Results Storage*: Store evaluation results in S3-compatible storage
. *Results Analysis*: Visualize and compare results using Jupyter notebooks

== Key Features

=== Multi-Language Support

Evaluate your models across three languages:

* *English (en)*: Standard benchmarks including ARC-Easy, HellaSwag, WinoGrande, TruthfulQA
* *Spanish (es)*: BELEBELE Spanish, XNLI Spanish
* *Japanese (ja)*: BELEBELE Japanese, XNLI Japanese

=== Automated Workflow

The entire pipeline is automated using Tekton on OpenShift, allowing you to:

* Run evaluations with a single command
* Skip specific steps as needed
* Track progress through the OpenShift Console
* Store and compare results over time

=== Flexible Configuration

* Enable/disable specific evaluation types
* Configure S3 storage for results
* Adjust vLLM parameters for your hardware
* Choose which languages to evaluate

== Pipeline Components

The pipeline consists of several integrated components working together:

1. *Model Download*: Pulls models from HuggingFace Hub
2. *Model Compression*: Optional weight quantization for efficiency
3. *Modelcar Packaging*: Creates OCI container images with embedded models
4. *Model Registry*: Manages model versions and metadata
5. *vLLM Deployment*: Serves models with high-performance inference
6. *Multi-Language Evaluation*: Runs benchmarks across languages
7. *TrustyAI Integration*: Tests for bias and fairness (optional)
8. *S3 Storage*: Persists results for historical comparison
9. *Jupyter Analysis*: Visualizes performance across languages

== Who Should Use This

This pipeline is designed for:

* *ML Engineers*: Evaluating model performance across languages
* *AI Platform Teams*: Standardizing model evaluation workflows
* *Data Scientists*: Comparing model variants and optimizations
* *Quality Assurance*: Testing models for bias and fairness

== Next Steps

. Review the xref:architecture.adoc[Pipeline Architecture]
. Check xref:prerequisites.adoc[Prerequisites] for your environment
. Follow the xref:chapter1:index.adoc[Model Preparation] guide to get started

== Support and Contribution

For issues, questions, or contributions, contact the Red Hat AI Customer Adoption and Innovation team (CAI).
