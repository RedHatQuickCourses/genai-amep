= Results Analysis and Visualization
:navtitle: Results Analysis
:description: Using RHOAI Workbenches and Jupyter Notebooks to visualize and analyze evaluation results.
:page-role: data-scientist,platform-engineer

[.lead]
After running evaluations, you need to make sense of the data. This guide shows you how to use RHOAI Workbenches and Jupyter Notebooks to visualize, analyze, and report on your model evaluation results.

== Prerequisites

Before starting, ensure you have:

* xref:infrastructure-setup.adoc[Infrastructure Setup] completed
* Evaluation results available (either from xref:modelcar-pipeline.adoc[ModelCar Pipeline] or xref:evaluation-pipeline.adoc[Evaluation Pipeline])
* Access to RHOAI Workbenches with Jupyter Notebook support
* Results stored in S3 (MinIO) or accessible via PVC

== Overview

The AMEP solution provides two Jupyter Notebooks for results analysis:

. **HumanEval Analysis** (`humaneval.ipynb`): Interactive evaluation of code generation capabilities
. **Multi-Language Evaluation Analysis** (`multilang_evaluation_analysis.ipynb`): Comprehensive visualization and comparison of model performance across languages

== Setting Up RHOAI Workbench

=== Create a Workbench Instance

. Navigate to **OpenShift AI Dashboard** → **Workbenches**
. Click **Create workbench**
. Configure the workbench:
   * **Name**: `evaluation-analysis`
   * **Image**: Select a Python-based image (e.g., `Standard Data Science` or `PyTorch`)
   * **Container size**: Medium or Large (depending on your data size)
   * **Storage**: Attach a PVC if you need to access evaluation results from a shared volume

=== Access the Notebook Environment

. Once the workbench is running, click **Open** to launch JupyterLab
. In JupyterLab, open a terminal and clone the repository:

[source,bash]
----
cd ~
git clone https://github.com/RedHatQuickCourses/genai-amep.git
cd genai-amep/deploy/notebooks
----

== HumanEval Analysis Notebook

The HumanEval notebook provides an interactive way to evaluate code generation capabilities of your deployed models.

=== Running HumanEval Evaluation

. Open `humaneval.ipynb` in JupyterLab
. The notebook contains the following sections:

=== Launch vLLM Server (Optional)

If you need to run a local vLLM server for testing:

[source,python]
----
import subprocess

# Define your vLLM launch command
vllm_command = [
    "python",
    "-m", "vllm.entrypoints.openai.api_server",
    "--model", "RedHatAI/Meta-Llama-3.1-8B-Instruct-quantized.w4a16",
    "--port", "8000",
    "--max-model-len", "5000",
    "--dtype", "float16"
]

# Run it in the background
vllm_process = subprocess.Popen(vllm_command)
print("✅ vLLM server started on http://localhost:8000")
----

[NOTE]
====
If you're evaluating a model already deployed via InferenceService, skip this step and point the client to your deployed endpoint.
====

=== Connect to Model Endpoint

Configure the OpenAI client to connect to your model:

[source,python]
----
from openai import OpenAI

# For local vLLM server
client = OpenAI(base_url="http://localhost:8000/v1", api_key="EMPTY")

# For deployed InferenceService
# client = OpenAI(
#     base_url="http://<MODEL_NAME>-predictor.<NAMESPACE>.svc.cluster.local:8080/v1",
#     api_key="EMPTY"
# )
----

=== Run HumanEval Evaluation

The notebook will:

. Clone the HumanEval repository
. Install dependencies
. Iterate through all 164 HumanEval problems
. Test each generated solution for correctness
. Display real-time pass@1 metrics

[source,python]
----
import json
from human_eval.data import read_problems
from human_eval.execution import check_correctness

# Load HumanEval prompts
problems = read_problems()

results = []
for idx, (task_id, task) in enumerate(problems.items(), start=1):
    prompt = task["prompt"].lstrip()
    
    response = client.completions.create(
        model="your-model-name",
        prompt=prompt,
        max_tokens=512,
        temperature=0.0,
        top_p=1.0,
        stop=["\nclass", "\ndef", "\n#"]
    )
    
    completion = response.choices[0].text
    result = check_correctness(problem=task, completion=completion, timeout=3)
    
    results.append({
        "task_id": task_id,
        "passed": result["passed"],
        "result": result["result"]
    })
    
    # Display progress
    pass_count = sum(r["passed"] for r in results)
    running_pass_rate = pass_count / idx
    print(f"{task_id}: {'✅' if result['passed'] else '❌'} | "
          f"running pass@1: {running_pass_rate:.3f} ({pass_count}/{idx})")

# Final summary
total = len(results)
final_pass_rate = pass_count / total
print(f"\nFinal Pass@1: {final_pass_rate:.3f} ({pass_count}/{total})")
----

=== Export Results

Save the evaluation results for later analysis:

[source,python]
----
# Save results to JSON
with open('humaneval_results.json', 'w') as f:
    json.dump(results, f, indent=2)

# Or upload to S3
import boto3
s3 = boto3.client('s3',
    endpoint_url='http://minio-service.s3-storage.svc.cluster.local:9000',
    aws_access_key_id='minio',
    aws_secret_access_key='minio123'
)
s3.upload_file('humaneval_results.json', 'benchmark-results', 
               'humaneval_results.json')
----

== Multi-Language Evaluation Analysis

The multi-language evaluation analysis notebook provides comprehensive visualization and comparison capabilities for evaluation results across multiple languages.

=== Loading Evaluation Results

The notebook supports loading results from two sources:

==== Local Storage (PVC)

If results are stored in a shared PVC:

[source,python]
----
# Configuration
DATA_SOURCE = 'local'
LOCAL_RESULTS_DIR = Path("/workspace/shared-workspace/multilang_evaluation_results")

# Load results
evaluation_results = load_local_results(LOCAL_RESULTS_DIR)
----

==== S3 Storage (MinIO)

If results are stored in S3:

[source,python]
----
# Configuration
DATA_SOURCE = 's3'
S3_CONFIG = {
    'bucket': 'llm-evaluation-results',
    'endpoint_url': 'http://minio-service.s3-storage.svc.cluster.local:9000',
    'access_key': os.environ.get('AWS_ACCESS_KEY_ID', 'minio'),
    'secret_key': os.environ.get('AWS_SECRET_ACCESS_KEY', 'minio123'),
}

# Load results
s3_prefix = "evaluations/model_name/version/timestamp"
evaluation_results = load_s3_results(s3_prefix, S3_CONFIG)
----

=== Visualizations

The notebook generates several types of visualizations:

==== Performance Comparison Chart

Compare model performance across languages using an interactive bar chart:

[source,python]
----
import plotly.express as px

# Filter for accuracy metrics
df_accuracy = df_metrics[
    df_metrics['metric'].str.contains('acc|f1|em', case=False, na=False)
]

# Create bar chart
fig = px.bar(
    df_accuracy,
    x='task',
    y='value',
    color='language_name',
    barmode='group',
    title='Model Performance Comparison Across Languages',
    labels={'value': 'Score', 'task': 'Benchmark Task'},
    height=500
)
fig.show()
----

==== Performance Heatmap

Visualize performance across all tasks and languages:

[source,python]
----
import seaborn as sns
import matplotlib.pyplot as plt

# Aggregate metrics
heatmap_data = df_accuracy.groupby(['task', 'language_name'])['value'].mean().unstack()

# Create heatmap
plt.figure(figsize=(10, 6))
sns.heatmap(
    heatmap_data,
    annot=True,
    fmt='.3f',
    cmap='RdYlGn',
    cbar_kws={'label': 'Score'},
    vmin=0,
    vmax=1
)
plt.title('Performance Heatmap: Tasks vs Languages', fontsize=14)
plt.tight_layout()
plt.show()
----

==== Performance Gap Analysis

Analyze performance gaps relative to a baseline language (typically English):

[source,python]
----
# Calculate performance gap relative to English
if 'English' in heatmap_data.columns:
    performance_gaps = pd.DataFrame()
    
    for lang in heatmap_data.columns:
        if lang != 'English':
            gaps = (heatmap_data[lang] - heatmap_data['English']) / heatmap_data['English'] * 100
            performance_gaps[lang] = gaps
    
    # Plot performance gaps
    performance_gaps.plot(kind='bar', figsize=(10, 6))
    plt.title('Performance Gap Relative to English (%)')
    plt.xlabel('Benchmark Task')
    plt.ylabel('Performance Gap (%)')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    plt.show()
    
    # Display average gaps
    avg_gaps = performance_gaps.mean()
    for lang, gap in avg_gaps.items():
        print(f"{lang}: {gap:+.2f}%")
----

=== Generating Reports

The notebook can generate comprehensive markdown reports:

[source,python]
----
# Generate markdown report
report = []
report.append("# Multi-Language LLM Evaluation Report\n")
report.append(f"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

# Add model information
if evaluation_results['summary']:
    summary = evaluation_results['summary']
    report.append("## Model Information\n")
    report.append(f"- **Model**: {summary.get('model_name', 'Unknown')}\n")
    report.append(f"- **Version**: {summary.get('model_version', 'Unknown')}\n")
    report.append(f"- **Evaluation Date**: {summary.get('timestamp', 'Unknown')}\n")

# Add performance summary
report.append("\n## Performance Summary\n")
report.append(f"\n{pivot_table.to_markdown()}\n")

# Save report
report_file = Path("multilang_evaluation_report.md")
with open(report_file, 'w') as f:
    f.writelines(report)

print(f"✅ Report saved to: {report_file}")
----

=== Exporting Results

Export analysis results in multiple formats:

[source,python]
----
# Export to CSV
pivot_table.to_csv("multilang_evaluation_summary.csv")

# Export to JSON
with open("evaluation_analysis.json", 'w') as f:
    json.dump({
        'summary': evaluation_results['summary'],
        'metrics': df_metrics.to_dict('records'),
        'heatmap_data': heatmap_data.to_dict()
    }, f, indent=2)

# Upload to S3
s3.upload_file("multilang_evaluation_summary.csv", 
               'benchmark-results', 
               'analysis/multilang_summary.csv')
----

== Comparing Multiple Evaluation Runs

To compare results across multiple model versions or evaluation runs:

[source,python]
----
# Load multiple evaluation runs
evaluation_dirs = [
    Path("/workspace/shared-workspace/multilang_evaluation_results_v1"),
    Path("/workspace/shared-workspace/multilang_evaluation_results_v2"),
]

all_results = {}
for eval_dir in evaluation_dirs:
    version = eval_dir.name.split('_')[-1]
    all_results[version] = load_local_results(eval_dir)

# Create comparison DataFrame
comparison_data = []
for version, results in all_results.items():
    df = extract_metrics(results)
    df['version'] = version
    comparison_data.append(df)

df_comparison = pd.concat(comparison_data, ignore_index=True)

# Visualize comparison
fig = px.bar(
    df_comparison,
    x='task',
    y='value',
    color='version',
    facet_col='language_name',
    title='Model Performance Comparison Across Versions',
    height=600
)
fig.show()
----

== Accessing Results from S3

If your evaluation results are stored in S3 (MinIO), you can access them directly:

=== Configure S3 Access

[source,python]
----
import boto3
from botocore.client import Config

# Configure S3 client for MinIO
s3_client = boto3.client(
    's3',
    endpoint_url='http://minio-service.s3-storage.svc.cluster.local:9000',
    aws_access_key_id='minio',
    aws_secret_access_key='minio123',
    config=Config(signature_version='s3v4'),
    use_ssl=False
)

# List available evaluation results
bucket = 'benchmark-results'
prefix = 'evaluations/'
response = s3_client.list_objects_v2(Bucket=bucket, Prefix=prefix)

for obj in response.get('Contents', []):
    print(f"  {obj['Key']} ({obj['Size']} bytes)")
----

=== Download Results

[source,python]
----
# Download specific evaluation results
s3_key = "evaluations/granite-2b/1.0.0/20240101_120000/en/results.json"
local_path = "/tmp/evaluation_results.json"

s3_client.download_file(bucket, s3_key, local_path)

# Load and analyze
with open(local_path) as f:
    results = json.load(f)
----

== Best Practices

=== Organizing Results

* Use consistent naming conventions for evaluation runs
* Include timestamps in directory/file names
* Store metadata (model name, version, date) with results
* Keep raw results separate from processed/aggregated data

=== Visualization Guidelines

* Use consistent color schemes across visualizations
* Include clear axis labels and legends
* Add context (model name, version, date) to all charts
* Export high-resolution images for reports

=== Performance Considerations

* For large datasets, consider sampling or aggregation
* Cache intermediate results to speed up re-analysis
* Use appropriate data types (e.g., float32 vs float64)
* Profile notebook execution to identify bottlenecks

== Troubleshooting

=== Notebook Fails to Load Results

[source,bash]
----
# Verify file paths
ls -la /workspace/shared-workspace/multilang_evaluation_results/

# Check file permissions
chmod -R 755 /workspace/shared-workspace/

# Verify JSON format
python -m json.tool /path/to/results.json
----

=== S3 Connection Issues

[source,python]
----
# Test S3 connectivity
try:
    s3_client.list_buckets()
    print("✅ S3 connection successful")
except Exception as e:
    print(f"❌ S3 connection failed: {e}")
    print("Check:")
    print("  - MinIO service is running")
    print("  - Endpoint URL is correct")
    print("  - Credentials are valid")
    print("  - Network connectivity from workbench to MinIO")
----

=== Missing Dependencies

[source,bash]
----
# Install required packages
pip install pandas matplotlib seaborn plotly boto3

# Or use the notebook's installation cell
!pip install -q pandas matplotlib seaborn plotly boto3
----

== Next Steps

* xref:modelcar-pipeline.adoc[Package and Deploy Models] - Create new model versions to evaluate
* xref:evaluation-pipeline.adoc[Run Evaluations] - Generate new evaluation results
* Integrate analysis notebooks into CI/CD workflows
* Create custom visualizations for domain-specific metrics
* Set up automated report generation

