= ModelCar Pipeline Deployment

This guide covers the deployment and usage of the ModelCar pipeline, which automates the downloading, storage, and registration of AI models in OpenShift AI.

== Overview

The ModelCar pipeline automates the workflow of downloading models from Hugging Face, storing them in S3-compatible storage, and registering them in the RHOAI Model Registry. It uses Tekton to orchestrate the following tasks:

. Clean up workspace
. Download model from Hugging Face
. Upload model to S3 storage (under `model-data/` folder)
. Register model in RHOAI Model Registry


== Prerequisites

Before deploying the ModelCar pipeline, ensure you have completed:

* xref:infrastructure-setup.adoc[Infrastructure Setup] - S3 Storage and Model Registry deployed
* Pipeline namespace created (e.g., `modelcar-pipelines`)
* Secrets configured:
** `s3-credentials` - S3 access credentials (AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY)
** `huggingface-secret` - Hugging Face token (for gated models, optional)

== Deploy Pipeline Tasks

The ModelCar pipeline consists of several reusable Tekton tasks.

=== Create Pipeline Storage

Create a PersistentVolumeClaim for pipeline workspace:

[source,bash]
----
oc apply -f deploy/openshift/modelcar-storage.yaml -n modelcar-pipelines
----

This creates a PVC named `modelcar-storage` for storing model files during pipeline execution.

=== Create S3 Credentials Secret

The pipeline requires S3 credentials for uploading models:

[source,bash]
----
# Create S3 credentials secret
oc create secret generic s3-credentials \
  --from-literal=AWS_ACCESS_KEY_ID=<YOUR_ACCESS_KEY> \
  --from-literal=AWS_SECRET_ACCESS_KEY=<YOUR_SECRET_KEY> \
  -n modelcar-pipelines
----

=== Create Required ConfigMaps

The pipeline requires a ConfigMap for the S3 registration script:

[source,bash]
----
# Create S3 registration script ConfigMap
oc create configmap register-s3-script \
  --from-file=register_s3_model.py=deploy/tasks/register-with-registry/register_s3_model.py \
  -n modelcar-pipelines
----

=== Create ServiceAccount and RBAC

The pipeline needs a ServiceAccount with proper permissions for accessing the workspace, secrets, and Model Registry:

[source,bash]
----
# Create ServiceAccount with RBAC permissions
oc apply -f deploy/openshift/modelcar-rbac.yaml
----

This creates:

* ServiceAccount: `modelcar-pipeline`
* Role: `modelcar-pipeline-role` (namespace-scoped permissions)
* RoleBinding: `modelcar-pipeline-binding`
* ClusterRole: `modelcar-model-registry-access` (for cross-namespace Model Registry access)
* ClusterRoleBinding: `modelcar-model-registry-access`

[NOTE]
====
The ModelCar pipeline stores models in S3 and registers them in the Model Registry. For model deployment and evaluation, use the xref:benchmark-pipeline.adoc[Benchmark Pipeline].
====

=== Deploy Main Pipeline

Deploy the main ModelCar pipeline:

[source,bash]
----
oc apply -f deploy/openshift/modelcar-pipeline.yaml -n modelcar-pipelines
----

==== Verify Pipeline Deployment

Check that the pipeline and all components are created:

[source,bash]
----
# Check pipeline
oc get pipelines -n modelcar-pipelines

# Check ConfigMaps
oc get configmaps -n modelcar-pipelines | grep register-s3-script

# Check ServiceAccount and RBAC
oc get serviceaccount modelcar-pipeline -n modelcar-pipelines
oc get role modelcar-pipeline-role -n modelcar-pipelines
oc get clusterrole modelcar-model-registry-access

# Check storage
oc get pvc modelcar-storage -n modelcar-pipelines

# Check secrets
oc get secret s3-credentials -n modelcar-pipelines
----

== Running the ModelCar Pipeline

=== Basic Pipeline Run

To download a model from Hugging Face and store it in S3:

[source,bash]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: modelcar-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: modelcar-pipeline
  params:
    - name: HUGGINGFACE_MODEL
      value: "ibm-granite/granite-3.3-2b-instruct"
    - name: MODEL_NAME
      value: "granite-3.3-2b-instruct"
    - name: MODEL_VERSION
      value: "1.0.0"
    - name: MODEL_REGISTRY_URL
      value: "http://mysql.rhoai-model-registries.svc.cluster.local:3306"
    - name: S3_BUCKET
      value: "models"
    - name: S3_ENDPOINT_URL
      value: "http://minio.minio.svc.cluster.local:9000"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: modelcar-storage
    - name: s3-credentials
      secret:
        secretName: s3-credentials
EOF
----

NOTE: The model will be uploaded to S3 at the path: `s3://models/model-data/granite-3.3-2b-instruct/1.0.0/`

=== Pipeline with Gated Model

For gated models requiring authentication, first create the HuggingFace secret:

[source,bash]
----
# Create HuggingFace token secret
oc create secret generic huggingface-secret \
  --from-literal=HUGGINGFACE_TOKEN=<YOUR_HF_TOKEN> \
  -n modelcar-pipelines
----

Then run the pipeline:

[source,bash]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: modelcar-gated-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: modelcar-pipeline
  params:
    - name: HUGGINGFACE_MODEL
      value: "meta-llama/Llama-3.2-3B-Instruct"
    - name: MODEL_NAME
      value: "llama-3.2-3b-instruct"
    - name: MODEL_VERSION
      value: "1.0.0"
    - name: MODEL_REGISTRY_URL
      value: "http://mysql.rhoai-model-registries.svc.cluster.local:3306"
    - name: S3_BUCKET
      value: "models"
    - name: S3_ENDPOINT_URL
      value: "http://minio.minio.svc.cluster.local:9000"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: modelcar-storage
    - name: s3-credentials
      secret:
        secretName: s3-credentials
EOF
----

NOTE: The `huggingface-secret` must contain a valid token with access to the gated model.

== Pipeline Parameters Reference

The ModelCar pipeline supports the following parameters:

[cols="1,2,1,2"]
|===
|Parameter |Description |Required |Default

|`HUGGINGFACE_MODEL`
|Hugging Face model repository (e.g., `ibm-granite/granite-3.3-2b-instruct`)
|Yes
|N/A

|`HUGGINGFACE_ALLOW_PATTERNS`
|File patterns to download (e.g., `"*.safetensors", "*.json"`)
|No
|`*.safetensors, *.json, *.txt`

|`COMPRESS_MODEL`
|Whether to compress the model (`true`/`false`)
|No
|`false`

|`MODEL_NAME`
|Name to register in model registry
|Yes
|N/A

|`MODEL_VERSION`
|Version to register in model registry
|No
|`1.0.0`

|`MODEL_REGISTRY_URL`
|URL of the model registry service
|Yes
|N/A

|`S3_BUCKET`
|S3 bucket name for model storage
|Yes
|`models`

|`S3_ENDPOINT_URL`
|S3 endpoint URL (for MinIO or S3-compatible storage)
|No
|``

|`DEPLOY_MODEL`
|Whether to deploy as InferenceService (`true`/`false`)
|No
|`false`

|`EVALUATE_MODEL`
|Whether to run evaluation (`true`/`false`)
|No
|`false`

|`TASKS`
|Comma-separated list of evaluation tasks
|No
|`humaneval,mbpp`

|`GUIDELLM_EVALUATE_MODEL`
|Whether to run GuideLLM evaluation (`true`/`false`)
|No
|`false`

|`MULTILANG_EVALUATE_MODEL`
|Whether to run multi-language evaluation (`true`/`false`)
|No
|`false`

|`LANGUAGES`
|Comma-separated list of languages to evaluate (`en,es,ja`)
|No
|`en,es,ja`

|`TRUSTYAI_EVALUATE_MODEL`
|Whether to run TrustyAI evaluation (`true`/`false`)
|No
|`false`

|`TRUSTYAI_SERVICE_URL`
|TrustyAI service endpoint URL
|No
|`http://trustyai-service:8080`

|`S3_ENABLED`
|Whether to upload results to S3 (`true`/`false`)
|No
|`false`

|`SKIP_TASKS`
|Comma-separated list of tasks to skip
|No
|``

|`MAX_MODEL_LEN`
|Maximum model length for vLLM
|No
|`8192`

|`VLLM_ARGS`
|Additional vLLM arguments
|No
|``
|===

== Monitoring Pipeline Execution

=== View Pipeline Run Status

[source,bash]
----
# List all pipeline runs
oc get pipelinerun -n modelcar-pipelines

# Watch a specific pipeline run
oc get pipelinerun <PIPELINERUN_NAME> -n modelcar-pipelines -w

# Describe pipeline run for details
oc describe pipelinerun <PIPELINERUN_NAME> -n modelcar-pipelines
----

=== View Task Logs

[source,bash]
----
# List task runs
oc get taskrun -n modelcar-pipelines

# View logs for a specific task
oc logs -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -l tekton.dev/pipelineTask=<TASK_NAME> \
  -n modelcar-pipelines

# Examples:
# View download task logs
oc logs -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -l tekton.dev/pipelineTask=pull-model-from-huggingface \
  -n modelcar-pipelines

# View compression task logs
oc logs -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -l tekton.dev/pipelineTask=compress-model \
  -n modelcar-pipelines
----

=== Using OpenShift Console

. Navigate to Pipelines â†’ PipelineRuns in the OpenShift Console
. Select your namespace (`modelcar-pipelines`)
. Click on a pipeline run to view details
. View logs for each task in the visual pipeline graph

== Container Images

The pipeline uses several custom container images that must be built and pushed to your registry:

=== Hugging Face Model Downloader

Location: `deploy/tasks/huggingface-modelcar-builder/`

[source,bash]
----
cd deploy/tasks/huggingface-modelcar-builder
podman build --platform linux/amd64 \
  -t quay.io/<YOUR_ORG>/huggingface-modelcar-builder:latest \
  -f Containerfile .
podman push quay.io/<YOUR_ORG>/huggingface-modelcar-builder:latest
----

=== S3 Uploader

Location: `deploy/tasks/s3-integration/`

[source,bash]
----
cd deploy/tasks/s3-integration
podman build --platform linux/amd64 \
  -t quay.io/<YOUR_ORG>/s3-uploader:latest \
  -f Containerfile .
podman push quay.io/<YOUR_ORG>/s3-uploader:latest
----

=== Model Registry Integration

Location: `deploy/tasks/register-with-registry/`

[source,bash]
----
cd deploy/tasks/register-with-registry
podman build --platform linux/amd64 \
  -t quay.io/<YOUR_ORG>/modelcar-register:latest \
  -f Containerfile .
podman push quay.io/<YOUR_ORG>/modelcar-register:latest
----

NOTE: Update the image references in the pipeline YAML files to point to your custom images.

== Troubleshooting

=== Pipeline Run Fails at Download Step

Check Hugging Face token and model access:

[source,bash]
----
# Verify secret exists
oc get secret huggingface-secret -n modelcar-pipelines

# Check if model is gated
# Visit https://huggingface.co/<MODEL_REPO> and verify access
----

=== Pipeline Run Fails at S3 Upload Step

Check S3 credentials and connectivity:

[source,bash]
----
# Verify S3 secret exists
oc get secret s3-credentials -n modelcar-pipelines

# Test S3 connectivity (from a test pod)
oc run s3-test --rm -it --image=amazon/aws-cli -n modelcar-pipelines -- \
  s3 ls s3://<BUCKET_NAME> --endpoint-url=<S3_ENDPOINT_URL>
----

=== Model Registry Connection Issues

Verify Model Registry connectivity:

[source,bash]
----
# Check MySQL service
oc get svc mysql -n rhoai-model-registries

# Test connection from pipeline namespace
oc run test-mysql --rm -it --image=mysql:8.0 -n modelcar-pipelines -- \
  mysql -h mysql.rhoai-model-registries.svc.cluster.local \
  -u <USERNAME> -p<PASSWORD> -e "SHOW DATABASES;"
----

=== Out of Memory Errors

Increase memory limits in pipeline task specs or use compression:

[source,bash]
----
# Edit pipeline to increase memory
oc edit pipeline modelcar-pipeline -n modelcar-pipelines

# Or set COMPRESS_MODEL=true to reduce memory usage
----

== Next Steps

* xref:benchmark-pipeline.adoc[Deploy Benchmark Pipeline] - Set up model evaluation and benchmarking workflows
* Integrate with CI/CD workflows
* View registered models in the RHOAI Model Registry
