= Evaluation Pipeline Deployment

This guide covers the deployment and usage of the GuideLLM evaluation pipeline for comprehensive model assessment on OpenShift AI.

== Overview

The evaluation pipeline provides automated testing and benchmarking of deployed models. It works in conjunction with the xref:modelcar-pipeline.adoc[ModelCar Pipeline] to evaluate models that have been packaged and optionally deployed.

The evaluation pipeline consists of multiple integrated evaluation tasks:

* *Model Deployment* (optional): Deploys models using Helm if not already deployed
* *GuideLLM Benchmarking*: Performance, throughput, and latency testing
* *LM-Eval Tasks*: Accuracy evaluation using TrustyAI LMEvalJob (MMLU, HumanEval, MBPP, etc.)
* *Custom Evaluation Tasks*: Support for custom evaluation datasets
* *S3 Storage*: Automated upload of results to object storage
* *Model Registry Integration*: Automatic registration of evaluation results

[NOTE]
====
**Relationship to ModelCar Pipeline:**
* The ModelCar pipeline packages models into OCI images and can deploy them
* The evaluation pipeline evaluates already-deployed models OR can deploy them first
* Both pipelines can work independently, but typically you:
  1. Package a model using ModelCar pipeline
  2. Evaluate the deployed model using this evaluation pipeline
  3. Results are stored in S3 and registered in the Model Registry
====

== Prerequisites

Before deploying the evaluation pipeline, ensure you have:

* xref:infrastructure-setup.adoc[Infrastructure Setup] completed (MinIO, Model Registry)
* xref:modelcar-pipeline.adoc[ModelCar Pipeline] deployed and a model packaged
* OpenShift AI 3.0+ with TrustyAI operator installed (for LMEvalJob support)
* A deployed model (InferenceService) to evaluate, or the pipeline will deploy one
* Pipeline namespace configured (e.g., `modelcar-pipelines`)
* A separate namespace for model deployments (e.g., `vllm`) - this is where LMEvalJobs run
* ServiceAccount named `pipeline` in the pipeline namespace with appropriate permissions

== Pipeline Architecture

The evaluation pipeline orchestrates the following workflow:

[source,mermaid]
----
graph LR
    A[Deploy Model] --> B[GuideLLM Benchmark]
    B --> C[Upload GuideLLM Results]
    C --> D[LM-Eval Tasks]
    D --> E[Upload LM-Eval Results]
    E --> F[Register Results in Model Registry]
----

. *Deploy Model*: Optionally deploys the model as a vLLM InferenceService using Helm (if not already deployed)
. *GuideLLM Benchmark*: Runs performance benchmarking against the model endpoint
. *Upload GuideLLM Results*: Stores benchmark results in S3 (bucket: `guidellm-results`)
. *LM-Eval Tasks*: Creates and runs TrustyAI LMEvalJob for accuracy evaluation
. *Upload LM-Eval Results*: Stores evaluation results in S3 (bucket: `lm-eval-results`)
. *Register Results*: Updates model registry with evaluation metadata and links to results

== Deploy Pipeline Components

=== Create Pipeline Storage

Create a PVC for storing evaluation results:

[source,bash]
----
oc apply -f deploy/guidellm-pipeline/pipeline/pvc.yaml -n modelcar-pipelines
----

This creates `guidellm-output-pvc` for workspace storage.

=== Deploy Evaluation Tasks

Deploy the individual Tekton tasks:

[source,bash]
----
# Deploy model task
oc apply -f deploy/guidellm-pipeline/pipeline/deploy-model-task.yaml -n modelcar-pipelines

# GuideLLM benchmark task
oc apply -f deploy/guidellm-pipeline/pipeline/guidellm-benchmark-task.yaml -n modelcar-pipelines

# Upload GuideLLM results task
oc apply -f deploy/guidellm-pipeline/pipeline/upload-guidellm-results-task.yaml -n modelcar-pipelines

# LM-Eval task
oc apply -f deploy/guidellm-pipeline/pipeline/lm-eval-task.yaml -n modelcar-pipelines

# Upload LM-Eval results task
oc apply -f deploy/guidellm-pipeline/pipeline/upload-lm-eval-results-task.yaml -n modelcar-pipelines

# Model registry integration task
oc apply -f deploy/guidellm-pipeline/pipeline/model-registry-task.yaml -n modelcar-pipelines
----

=== Create Evaluation ConfigMaps

The pipeline requires ConfigMaps for LM-Eval job definitions. Note that `mmlu.yaml` is actually a LMEvalJob Custom Resource, not a ConfigMap:

[source,bash]
----
# Create namespace for model deployments (if it doesn't exist)
oc create namespace vllm --dry-run=client -o yaml | oc apply -f -

# Create ConfigMap for standard MMLU evaluation job manifest
oc create configmap mmlu-manifest \
  --from-file=mmlu.yaml=deploy/guidellm-pipeline/pipeline/mmlu.yaml \
  -n modelcar-pipelines

# Create ConfigMap for custom evaluation (if using custom datasets)
oc create configmap custom-mmlu \
  --from-file=custom-mmlu.yaml=deploy/guidellm-pipeline/custom-lm-eval/custom-mmlu.yaml \
  --from-file=task.yaml=deploy/guidellm-pipeline/custom-lm-eval/task.yaml \
  --from-file=custom-task.yaml=deploy/guidellm-pipeline/custom-lm-eval/custom-task.yaml \
  --from-file=data.jsonl=deploy/guidellm-pipeline/custom-lm-eval/data.jsonl \
  -n modelcar-pipelines

# For custom evaluations, you may also need to create a ConfigMap with your custom data
# oc create configmap custom-lmeval-benchmark-files \
#   --from-file=task.yaml=deploy/guidellm-pipeline/custom-lm-eval/task.yaml \
#   --from-file=data.jsonl=deploy/guidellm-pipeline/custom-lm-eval/data.jsonl \
#   -n vllm
----

[NOTE]
====
The LMEvalJob Custom Resources are applied to the `vllm` namespace (or your model deployment namespace), not the pipeline namespace. The ConfigMaps are used by the pipeline to store the job definitions.
====

=== Deploy Main Pipeline

Deploy the main evaluation pipeline:

[source,bash]
----
oc apply -f deploy/guidellm-pipeline/pipeline/benchmark-eval-pipeline.yaml -n modelcar-pipelines
----

=== Create ServiceAccount

The pipeline requires a ServiceAccount with permissions to create and manage resources:

[source,bash]
----
# Create ServiceAccount (if not already exists)
oc create serviceaccount pipeline -n modelcar-pipelines --dry-run=client -o yaml | oc apply -f -

# Grant necessary permissions (adjust as needed for your environment)
# The ServiceAccount needs permissions to:
# - Create/delete LMEvalJobs in the vllm namespace
# - Access ConfigMaps and PVCs
# - Deploy models using Helm (if using deploy-model task)
----

=== Verify Pipeline Deployment

[source,bash]
----
# Check tasks
oc get tasks -n modelcar-pipelines | grep -E "(guidellm-benchmark|lm-eval|deploy-model|upload-guidellm|upload-lm-eval|model-registry)"

# Check pipeline
oc get pipeline guidellm-benchmark-pipeline -n modelcar-pipelines

# Check ConfigMaps
oc get configmap -n modelcar-pipelines | grep -E "(mmlu-manifest|custom-mmlu)"

# Check ServiceAccount
oc get serviceaccount pipeline -n modelcar-pipelines
----

== Running the Evaluation Pipeline

=== Basic Evaluation Run

To evaluate a deployed model:

[source,bash]
----
oc create -f - <<EOF
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: guidellm-benchmark-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: guidellm-benchmark-pipeline
  params:
    - name: target
      value: "http://granite-2b-predictor.vllm.svc.cluster.local:8080/v1"
    - name: model-name
      value: "granite-2b"
    - name: processor
      value: "ibm-granite/granite-3.3-2b-instruct"
    - name: data-config
      value: "prompt_tokens=800,output_tokens=128"
    - name: max-seconds
      value: "30"
    - name: rate-type
      value: "sweep"
    - name: rate
      value: "1.0, 4.0, 8.0, 16.0"
    - name: api-key
      value: ""
    - name: max-concurrency
      value: "10"
    - name: huggingface-token
      value: ""
    - name: s3-api-endpoint
      value: "http://minio-service.s3-storage.svc.cluster.local:9000"
    - name: s3-access-key-id
      value: "minio"
    - name: s3-secret-access-key
      value: "minio123"
    - name: model-url
      value: "quay.io/<YOUR_ORG>/granite-3.3-2b-instruct:latest"
    - name: lm-eval-job-name
      value: "mmlu-jurisprudence-eval-job"
    - name: lm-eval-custom
      value: "False"
    - name: custom-data
      value: "False"
    - name: custom-filename
      value: "no-file"
    - name: model-reg-author
      value: "ModelOps Team"
    - name: valuesContent
      value: |
        deploymentMode: RawDeployment
        fullnameOverride: "granite-2b"
        model:
          modelNameOverride: "granite-2b"
          uri: "oci://quay.io/<YOUR_ORG>/granite-3.3-2b-instruct:latest"
          args:
            - "--disable-log-requests"
            - "--max-num-seqs=32"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: guidellm-output-pvc
    - name: manifests
      configMap:
        name: mmlu-manifest
    - name: custom-mmlu
      configMap:
        name: custom-mmlu
  taskRunTemplate:
    serviceAccountName: pipeline
  timeouts:
    pipeline: 1h0m0s
EOF
----

[NOTE]
====
**Important Notes:**
* The `target` parameter should point to your deployed model's endpoint. If the model is not yet deployed, the `deploy-model` task will deploy it using the Helm chart specified in `valuesContent`.
* The `lm-eval-job-name` must match the name in the LMEvalJob CR (e.g., `mmlu-jurisprudence-eval-job` from `mmlu.yaml`).
* The LMEvalJob will be created in the `vllm` namespace (or the namespace specified in the job definition).
* Ensure the model endpoint URL format matches: `http://<SERVICE_NAME>-predictor.<NAMESPACE>.svc.cluster.local:8080/v1`
====

=== Custom Evaluation Run

To run custom evaluation tasks with custom data:

[source,bash]
----
# First, upload your custom data file to the S3 'custom-data' bucket
# Then create the PipelineRun:

oc create -f - <<EOF
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: custom-eval-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: guidellm-benchmark-pipeline
  params:
    - name: target
      value: "http://my-model-predictor.vllm.svc.cluster.local:8080/v1"
    - name: model-name
      value: "my-custom-model"
    - name: processor
      value: "org/model-repo"
    - name: data-config
      value: "prompt_tokens=1024,output_tokens=256"
    - name: max-seconds
      value: "60"
    - name: rate-type
      value: "fixed"
    - name: rate
      value: "10.0"
    - name: api-key
      value: ""
    - name: max-concurrency
      value: "10"
    - name: huggingface-token
      value: ""
    - name: s3-api-endpoint
      value: "http://minio-service.s3-storage.svc.cluster.local:9000"
    - name: s3-access-key-id
      value: "minio"
    - name: s3-secret-access-key
      value: "minio123"
    - name: model-url
      value: "quay.io/<YOUR_ORG>/my-custom-model:latest"
    - name: lm-eval-job-name
      value: "custom-eval-job"
    - name: lm-eval-custom
      value: "True"
    - name: custom-data
      value: "True"
    - name: custom-filename
      value: "my-custom-prompts.csv"
    - name: model-reg-author
      value: "ModelOps Team"
    - name: valuesContent
      value: |
        deploymentMode: RawDeployment
        fullnameOverride: "my-custom-model"
        model:
          modelNameOverride: "my-custom-model"
          uri: "oci://quay.io/<YOUR_ORG>/my-custom-model:latest"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: guidellm-output-pvc
    - name: manifests
      configMap:
        name: mmlu-manifest
    - name: custom-mmlu
      configMap:
        name: custom-mmlu
  taskRunTemplate:
    serviceAccountName: pipeline
  timeouts:
    pipeline: 2h0m0s
EOF
----

[NOTE]
====
**Custom Data Setup:**
* Upload your custom prompt file (CSV, JSONL, etc.) to the S3 bucket named `custom-data` in MinIO
* Set `custom-data: "True"` and provide the filename in `custom-filename`
* The file will be downloaded from S3 during the benchmark task
* For custom LM-Eval tasks, ensure your `custom-mmlu` ConfigMap contains the task definition and data files
====

== Pipeline Parameters Reference

[cols="1,2,1"]
|===
|Parameter |Description |Default

|`target`
|Model endpoint URL (e.g., `http://service.namespace.svc.cluster.local:8080/v1`)
|Required

|`model-name`
|Model identifier for results
|Required

|`processor`
|Hugging Face model path for tokenizer
|Required

|`data-config`
|Token configuration (`prompt_tokens=N,output_tokens=M`)
|`prompt_tokens=800,output_tokens=128`

|`max-seconds`
|Maximum benchmark duration in seconds
|`30`

|`rate-type`
|Benchmark rate type (`sweep`, `fixed`)
|`sweep`

|`rate`
|Request rate(s) for benchmarking
|`1.0, 4.0, 8.0, 16.0`

|`api-key`
|OpenAI API key if required
|``

|`max-concurrency`
|Maximum concurrent requests
|`10`

|`huggingface-token`
|Hugging Face token for gated models
|``

|`s3-api-endpoint`
|S3 endpoint URL (update to match your MinIO deployment)
|`http://s3.openshift-storage.svc.cluster.local:80` (default) or `http://minio-service.s3-storage.svc.cluster.local:9000` (MinIO)

|`s3-access-key-id`
|S3 access key ID
|Required

|`s3-secret-access-key`
|S3 secret access key
|Required

|`model-url`
|ModelCar OCI image URL
|Required

|`lm-eval-job-name`
|LM-Eval job name to run
|Required

|`lm-eval-custom`
|Whether to use custom evaluation (`True`/`False`)
|`False`

|`custom-data`
|Whether custom data is provided (`True`/`False`)
|`False`

|`custom-filename`
|Custom evaluation data filename
|`no-file`

|`model-reg-author`
|Model registry author name
|Required

|`valuesContent`
|Helm values for model deployment
|Required
|===

== Building Custom Evaluation Containers

The evaluation pipeline uses custom containers that can be built and customized:

=== GuideLLM Evaluation Container

Location: `deploy/tasks/evaluate/`

[source,bash]
----
cd deploy/tasks/evaluate
chmod +x build-guidellm-container.sh

# Build with defaults
./build-guidellm-container.sh

# Or build manually
podman build --platform linux/amd64 \
  -t quay.io/<YOUR_ORG>/guidellm-ubi:latest \
  -f Containerfile .
podman push quay.io/<YOUR_ORG>/guidellm-ubi:latest
----

Update the task YAML to reference your custom image:

[source,yaml]
----
# In guidellm-benchmark-task.yaml
steps:
  - name: guidellm-evaluate
    image: quay.io/<YOUR_ORG>/guidellm-ubi:latest
----

== Monitoring Evaluation Runs

=== View Pipeline Status

[source,bash]
----
# List evaluation pipeline runs
oc get pipelinerun -l tekton.dev/pipeline=guidellm-benchmark-pipeline \
  -n modelcar-pipelines

# Watch specific run
oc get pipelinerun <PIPELINERUN_NAME> -n modelcar-pipelines -w

# Describe for details
oc describe pipelinerun <PIPELINERUN_NAME> -n modelcar-pipelines
----

=== View Task Logs

[source,bash]
----
# GuideLLM benchmark logs
oc logs -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -l tekton.dev/pipelineTask=benchmark \
  -n modelcar-pipelines -f

# LM-Eval task logs (this task creates LMEvalJob in vllm namespace)
oc logs -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -l tekton.dev/pipelineTask=lm-eval \
  -n modelcar-pipelines -f

# Check LMEvalJob pod logs in vllm namespace
oc logs -l job-name=<LMEVAL_JOB_NAME> -n vllm -f

# Model registry task logs
oc logs -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -l tekton.dev/pipelineTask=register-model-and-results \
  -n modelcar-pipelines -f
----

=== View Results in S3

Access evaluation results in MinIO:

[source,bash]
----
# Get MinIO route
oc get route minio-api -n s3-storage

# Log in to MinIO console with credentials
# Navigate to buckets:
# - guidellm-results/ - GuideLLM benchmark results
# - lm-eval-results/ - LM-Eval task results
----

== Understanding Evaluation Results

=== GuideLLM Results

GuideLLM produces JSON files with performance metrics:

[source,json]
----
{
  "model": "granite-3.3-2b-instruct",
  "metrics": {
    "throughput": {
      "requests_per_second": 15.2,
      "tokens_per_second": 1248.5
    },
    "latency": {
      "mean_ms": 125.3,
      "p50_ms": 110.2,
      "p95_ms": 180.5,
      "p99_ms": 250.1
    },
    "quality": {
      "time_to_first_token_ms": 45.2,
      "inter_token_latency_ms": 8.5
    }
  }
}
----

Key metrics:
* *Throughput*: Requests and tokens per second
* *Latency*: Response time percentiles
* *Quality*: Time to first token, inter-token latency

=== LM-Eval Results

LM-Eval produces accuracy scores for various tasks:

[source,json]
----
{
  "results": {
    "mmlu": {
      "acc": 0.652,
      "acc_stderr": 0.012
    },
    "humaneval": {
      "pass@1": 0.347
    },
    "mbpp": {
      "pass@1": 0.412
    }
  },
  "config": {
    "model": "granite-3.3-2b-instruct",
    "tasks": ["mmlu", "humaneval", "mbpp"]
  }
}
----

== Creating Custom Evaluation Tasks

=== Custom LM-Eval Task

Create a custom LMEvalJob definition. The pipeline uses TrustyAI's LMEvalJob Custom Resource:

[source,yaml]
----
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: custom-eval-job
  namespace: vllm
spec:
  model: local-completions  # or local-chat-completions for chat models
  taskList:
    taskNames:
      - custom-lmeval-task  # Your custom task name
  modelArgs:
    - name: model
      value: granite-2b  # Your InferenceService name
    - name: base_url
      value: http://granite-2b-predictor.vllm.svc.cluster.local:8080/v1/completions
    - name: include_path
      value: /opt/app-root/src/my_tasks  # Path where task.yaml is mounted
    - name: num_concurrent
      value: '1'
    - name: max_retries
      value: '6'
    - name: tokenized_requests
      value: 'False'
    - name: tokenizer
      value: ibm-granite/granite-3.3-8b-instruct
  logSamples: true
  batchSize: '1'
  allowOnline: true
  allowCodeExecution: false
  outputs:
    pvcManaged:
      size: 5Gi
  pod:
    volumes:
      - name: custom-benchmark-volume
        configMap:
          name: custom-lmeval-benchmark-files
    container:
      volumeMounts:
        - name: custom-benchmark-volume
          mountPath: /opt/app-root/src/my_tasks/task.yaml
          subPath: task.yaml
          readOnly: true
        - name: custom-benchmark-volume
          mountPath: /opt/app-root/src/my_tasks/data.jsonl
          subPath: data.jsonl
          readOnly: true
----

Create the ConfigMap with your custom task files:

[source,bash]
----
# Create ConfigMap with custom task definition and data
oc create configmap custom-lmeval-benchmark-files \
  --from-file=task.yaml=deploy/guidellm-pipeline/custom-lm-eval/task.yaml \
  --from-file=data.jsonl=deploy/guidellm-pipeline/custom-lm-eval/data.jsonl \
  -n vllm

# Update the custom-mmlu ConfigMap in pipeline namespace
oc create configmap custom-mmlu \
  --from-file=custom-mmlu.yaml=my-custom-eval-job.yaml \
  --from-file=task.yaml=deploy/guidellm-pipeline/custom-lm-eval/task.yaml \
  --from-file=custom-task.yaml=deploy/guidellm-pipeline/custom-lm-eval/custom-task.yaml \
  --from-file=data.jsonl=deploy/guidellm-pipeline/custom-lm-eval/data.jsonl \
  -n modelcar-pipelines \
  --dry-run=client -o yaml | oc apply -f -
----

=== Custom Dataset

Create a ConfigMap with custom evaluation data:

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: my-custom-dataset
  namespace: modelcar-pipelines
data:
  custom-data.json: |
    [
      {
        "input": "What is the capital of France?",
        "expected_output": "Paris"
      },
      {
        "input": "Explain quantum computing.",
        "expected_output": "Quantum computing uses quantum mechanics..."
      }
    ]
----

== Troubleshooting

=== Model Endpoint Not Accessible

Verify the InferenceService is running:

[source,bash]
----
# Check InferenceService
oc get inferenceservice -n <MODEL_NAMESPACE>

# Check predictor pod
oc get pods -l serving.kserve.io/inferenceservice=<MODEL_NAME> \
  -n <MODEL_NAMESPACE>

# Test endpoint
oc run curl-test --rm -it --image=curlimages/curl -- \
  curl http://<MODEL_SERVICE>.<NAMESPACE>.svc.cluster.local:8080/v1/models
----

=== GuideLLM Benchmark Fails

Check GuideLLM task logs for errors:

[source,bash]
----
# View detailed logs
oc logs -l tekton.dev/pipelineTask=benchmark \
  -l tekton.dev/pipelineRun=<PIPELINERUN_NAME> \
  -n modelcar-pipelines

# Common issues:
# - Model endpoint unreachable
# - Invalid data-config format
# - Timeout too short for large models
----

=== S3 Upload Fails

Verify MinIO credentials and connectivity:

[source,bash]
----
# Check MinIO service
oc get svc minio-service -n s3-storage

# Test S3 access
oc run aws-cli --rm -it --image=amazon/aws-cli -- \
  s3 ls --endpoint-url http://minio-service.s3-storage.svc.cluster.local:9000 \
  --no-verify-ssl
----

=== LM-Eval Job Fails

Check the LMEvalJob status and logs:

[source,bash]
----
# List LMEvalJobs (in vllm namespace, not pipeline namespace)
oc get lmevaljob -n vllm

# Check LMEvalJob status
oc describe lmevaljob <JOB_NAME> -n vllm

# Check the pod created by LMEvalJob
oc get pods -n vllm | grep <JOB_NAME>

# Check pod logs
oc logs <POD_NAME> -n vllm

# Common issues:
# - Missing Hugging Face token for gated models (set in LMEvalJob spec)
# - Insufficient memory for large models
# - Invalid task configuration in ConfigMap
# - Model endpoint not accessible from vllm namespace
# - Missing custom task files in ConfigMap
----

== Best Practices

=== Resource Allocation

* Allocate sufficient memory for evaluation tasks (8Gi+ for larger models)
* Set appropriate timeouts based on model size and evaluation complexity
* Use node selectors for GPU-enabled nodes if evaluating large models

=== Result Management

* Regularly clean up old evaluation results from S3
* Use versioning in model registry to track evaluation history
* Archive important evaluation runs for compliance

=== Performance Optimization

* Run multiple evaluation tasks in parallel when possible
* Use shorter evaluation runs during development
* Run comprehensive evaluations during production releases

== Next Steps

* Integrate evaluation pipeline with CI/CD workflows
* Create custom evaluation tasks for domain-specific testing
* Set up automated alerts based on evaluation metrics
* Build dashboards for evaluation result visualization
