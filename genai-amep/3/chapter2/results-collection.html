<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Results Collection &amp; Visualization :: Automated Multi Language LLM Evaluation Pipeline</title>
    <link rel="prev" href="domain-evaluation-lab.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Automated Multi Language LLM Evaluation Pipeline</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/changeme/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header><div class="body">
<div class="nav-container" data-component="genai-amep" data-version="3">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Introduction</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Platform preparation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/infrastructure-setup.html">Infrastructure setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/modelcar-pipeline.html">ModelCar Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/benchmark-pipeline.html">Benchmark Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/evaluation-pipeline.html">Evaluation Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/results-analysis.html">Results Analysis and Visualization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Model Evaluation Lab</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="multilang-evaluation-lab.html">Multi-Language Evaluation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="performance-evaluation-lab.html">Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="domain-evaluation-lab.html">Domain-Specific Evaluation</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="results-collection.html">Results Collection &amp; Visualization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Automated Multi-Language LLM Evaluation Pipeline</span>
    <span class="version">3</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">3</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a></li>
    <li><a href="index.html">Model Evaluation Lab</a></li>
    <li><a href="results-collection.html">Results Collection &amp; Visualization</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Results Collection &amp; Visualization</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph lead">
<p><strong>Data without visualization is just numbers. Visualization turns data into decisions.</strong></p>
</div>
<div class="paragraph">
<p>In this module, you will collect all your evaluation results from S3 storage and create comprehensive visualizations to compare model performance across accuracy, performance, and domain knowledge. This is the only module that uses Jupyter notebooks, as visual comparison is best done interactively.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_visualization_matters"><a class="anchor" href="#_why_visualization_matters"></a>Why Visualization Matters</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Evaluation results stored in S3 are valuable, but raw JSON files don&#8217;t tell a story. Visualization helps you:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Compare Models:</strong> See how different models perform side-by-side</p>
</li>
<li>
<p><strong>Identify Trends:</strong> Spot performance patterns across languages or domains</p>
</li>
<li>
<p><strong>Communicate Results:</strong> Create stakeholder-ready reports and dashboards</p>
</li>
<li>
<p><strong>Make Decisions:</strong> Use visual evidence to select the best model for production</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before starting, ensure:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>You have completed the previous evaluation labs:</p>
<div class="ulist">
<ul>
<li>
<p><a href="multilang-evaluation-lab.html" class="xref page">Multi-Language Evaluation</a></p>
</li>
<li>
<p><a href="performance-evaluation-lab.html" class="xref page">Performance Benchmarking</a></p>
</li>
<li>
<p><a href="domain-evaluation-lab.html" class="xref page">Domain-Specific Evaluation</a></p>
</li>
</ul>
</div>
</li>
<li>
<p>Results are stored in S3</p>
</li>
<li>
<p>You have access to a Jupyter notebook environment (OpenShift AI Workbench)</p>
</li>
<li>
<p>You know the S3 bucket and path where results are stored</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_1_access_jupyter_workbench"><a class="anchor" href="#_step_1_access_jupyter_workbench"></a>Step 1: Access Jupyter Workbench</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Open your OpenShift AI Workbench:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Get the workbench route
oc get route -n redhat-ods-applications | grep workbench

# Or access via OpenShift AI Dashboard
# Navigate to: Workbenches → Create Workbench → Jupyter</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create a new Jupyter notebook or use the provided evaluation analysis notebook.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_2_install_required_libraries"><a class="anchor" href="#_step_2_install_required_libraries"></a>Step 2: Install Required Libraries</h2>
<div class="sectionbody">
<div class="paragraph">
<p>In your Jupyter notebook, install the required Python libraries:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># Install required packages
!pip install -q pandas matplotlib seaborn plotly boto3 s3fs

import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import boto3
from botocore.exceptions import ClientError

# Set style
sns.set_style("whitegrid")
plt.rcParams['figure.figsize'] = (12, 6)

print("✅ Dependencies loaded successfully")</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_3_configure_s3_access"><a class="anchor" href="#_step_3_configure_s3_access"></a>Step 3: Configure S3 Access</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Configure S3 access in your notebook:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># S3 Configuration
S3_CONFIG = {
    'endpoint_url': os.environ.get('S3_ENDPOINT_URL', 'http://minio-service.s3-storage.svc.cluster.local:9000'),
    'bucket': os.environ.get('S3_BUCKET', 'model-evaluations'),
    'access_key': os.environ.get('AWS_ACCESS_KEY_ID', 'minio'),
    'secret_key': os.environ.get('AWS_SECRET_ACCESS_KEY', 'minio123'),
}

# Initialize S3 client
s3_client = boto3.client(
    's3',
    endpoint_url=S3_CONFIG['endpoint_url'],
    aws_access_key_id=S3_CONFIG['access_key'],
    aws_secret_access_key=S3_CONFIG['secret_key']
)

print(f"✅ S3 client initialized for bucket: {S3_CONFIG['bucket']}")</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Using Data Connections</div>
<div class="paragraph">
<p>If you&#8217;re using OpenShift AI Data Connections, you can mount the S3 connection directly to your workbench instead of using credentials in code.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_4_load_evaluation_results"><a class="anchor" href="#_step_4_load_evaluation_results"></a>Step 4: Load Evaluation Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Create functions to load results from S3:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python">def load_multilang_results(model_id: str, timestamp: str) -&gt; Dict:
    """Load multi-language evaluation results from S3"""
    results = {}
    languages = ['en', 'es', 'ja']

    for lang in languages:
        key = f"multilang/{model_id}/{timestamp}/{lang}/results.json"
        try:
            response = s3_client.get_object(
                Bucket=S3_CONFIG['bucket'],
                Key=key
            )
            results[lang] = json.loads(response['Body'].read())
            print(f"✅ Loaded {lang} results")
        except ClientError as e:
            print(f"⚠️  {lang} results not found: {e}")

    return results

def load_performance_results(model_id: str, timestamp: str) -&gt; Dict:
    """Load performance benchmark results from S3"""
    key = f"performance/{model_id}/{timestamp}/performance-results.yaml"
    try:
        response = s3_client.get_object(
            Bucket=S3_CONFIG['bucket'],
            Key=key
        )
        # Parse YAML (you may need to install pyyaml)
        import yaml
        results = yaml.safe_load(response['Body'].read())
        print("✅ Loaded performance results")
        return results
    except ClientError as e:
        print(f"⚠️  Performance results not found: {e}")
        return None

def load_domain_results(model_id: str, timestamp: str) -&gt; Dict:
    """Load domain-specific evaluation results from S3"""
    # List files in domain results directory
    prefix = f"domain/{model_id}/{timestamp}/"
    try:
        response = s3_client.list_objects_v2(
            Bucket=S3_CONFIG['bucket'],
            Prefix=prefix
        )

        results = {}
        for obj in response.get('Contents', []):
            if obj['Key'].endswith('.json'):
                file_response = s3_client.get_object(
                    Bucket=S3_CONFIG['bucket'],
                    Key=obj['Key']
                )
                results[obj['Key'].split('/')[-1]] = json.loads(file_response['Body'].read())

        print(f"✅ Loaded {len(results)} domain result files")
        return results
    except ClientError as e:
        print(f"⚠️  Domain results not found: {e}")
        return None

# Load your results (update with your model ID and timestamp)
MODEL_ID = "granite-4.0-micro_v1.0"
TIMESTAMP = "20240101_120000"  # Update with your timestamp

multilang_results = load_multilang_results(MODEL_ID, TIMESTAMP)
performance_results = load_performance_results(MODEL_ID, TIMESTAMP)
domain_results = load_domain_results(MODEL_ID, TIMESTAMP)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_5_create_multi_language_comparison_visualization"><a class="anchor" href="#_step_5_create_multi_language_comparison_visualization"></a>Step 5: Create Multi-Language Comparison Visualization</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Visualize accuracy across languages:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># Extract accuracy metrics
def extract_accuracy_metrics(results: Dict) -&gt; pd.DataFrame:
    """Extract accuracy metrics from multi-language results"""
    data = []

    for lang, lang_data in results.items():
        if 'results' in lang_data:
            for task, metrics in lang_data['results'].items():
                if isinstance(metrics, dict) and 'acc' in metrics:
                    data.append({
                        'language': lang,
                        'task': task,
                        'accuracy': metrics['acc']
                    })

    return pd.DataFrame(data)

# Create comparison chart
if multilang_results:
    df_accuracy = extract_accuracy_metrics(multilang_results)

    fig = px.bar(
        df_accuracy,
        x='task',
        y='accuracy',
        color='language',
        barmode='group',
        title='Model Accuracy Comparison Across Languages',
        labels={'accuracy': 'Accuracy Score', 'task': 'Benchmark Task'},
        height=500
    )

    fig.update_layout(
        xaxis_tickangle=-45,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )

    fig.show()</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_6_create_performance_dashboard"><a class="anchor" href="#_step_6_create_performance_dashboard"></a>Step 6: Create Performance Dashboard</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Visualize performance metrics:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># Extract performance metrics
def extract_performance_metrics(results: Dict) -&gt; pd.DataFrame:
    """Extract performance metrics from GuideLLM results"""
    # Parse GuideLLM results format
    # Adjust based on your actual results structure
    metrics = []

    if results and 'throughput' in results:
        metrics.append({
            'metric': 'Requests Per Second',
            'value': results['throughput'].get('rps', 0),
            'category': 'Throughput'
        })
        metrics.append({
            'metric': 'Tokens Per Second',
            'value': results['throughput'].get('tps', 0),
            'category': 'Throughput'
        })

    if results and 'latency' in results:
        metrics.append({
            'metric': 'Time to First Token (ms)',
            'value': results['latency'].get('ttft_ms', 0),
            'category': 'Latency'
        })
        metrics.append({
            'metric': 'Inter-Token Latency (ms)',
            'value': results['latency'].get('itl_ms', 0),
            'category': 'Latency'
        })

    return pd.DataFrame(metrics)

# Create performance dashboard
if performance_results:
    df_perf = extract_performance_metrics(performance_results)

    fig = px.bar(
        df_perf,
        x='metric',
        y='value',
        color='category',
        title='Model Performance Metrics',
        labels={'value': 'Metric Value', 'metric': 'Performance Metric'},
        height=400
    )

    fig.show()</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_7_create_comprehensive_comparison_report"><a class="anchor" href="#_step_7_create_comprehensive_comparison_report"></a>Step 7: Create Comprehensive Comparison Report</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Generate a comprehensive report comparing all metrics:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># Create comparison report
report_data = []

# Add multi-language accuracy summary
if multilang_results:
    for lang, lang_data in multilang_results.items():
        if 'results' in lang_data:
            avg_accuracy = sum(
                m.get('acc', 0) for m in lang_data['results'].values()
                if isinstance(m, dict) and 'acc' in m
            ) / len([m for m in lang_data['results'].values() if isinstance(m, dict) and 'acc' in m])

            report_data.append({
                'Category': 'Accuracy',
                'Metric': f'{lang.upper()} Average Accuracy',
                'Value': f'{avg_accuracy:.2%}',
                'Model': MODEL_ID
            })

# Add performance summary
if performance_results:
    if 'throughput' in performance_results:
        report_data.append({
            'Category': 'Performance',
            'Metric': 'Requests Per Second',
            'Value': f"{performance_results['throughput'].get('rps', 0):.2f}",
            'Model': MODEL_ID
        })

# Create report DataFrame
df_report = pd.DataFrame(report_data)

# Display report
print("\n" + "="*60)
print("COMPREHENSIVE EVALUATION REPORT")
print("="*60)
print(f"Model: {MODEL_ID}")
print(f"Evaluation Date: {TIMESTAMP}")
print("="*60)
print(df_report.to_string(index=False))
print("="*60)</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_8_export_visualization_report"><a class="anchor" href="#_step_8_export_visualization_report"></a>Step 8: Export Visualization Report</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Export your visualizations as HTML or PDF:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># Create comprehensive dashboard
from plotly.subplots import make_subplots

fig = make_subplots(
    rows=2, cols=2,
    subplot_titles=('Accuracy by Language', 'Performance Metrics',
                    'Domain Evaluation', 'Summary'),
    specs=[[{"type": "bar"}, {"type": "bar"}],
           [{"type": "table"}, {"type": "indicator"}]]
)

# Add charts (customize based on your data)
# ... add your visualizations ...

# Export as HTML
fig.write_html("evaluation-dashboard.html")
print("✅ Dashboard exported to evaluation-dashboard.html")

# Or export as PDF (requires additional libraries)
# fig.write_image("evaluation-dashboard.pdf")</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_comparing_multiple_models"><a class="anchor" href="#_comparing_multiple_models"></a>Comparing Multiple Models</h2>
<div class="sectionbody">
<div class="paragraph">
<p>To compare multiple models, load results for each and create side-by-side comparisons:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-python hljs" data-lang="python"># Load results for multiple models
models = [
    {"id": "granite-4.0-micro_v1.0", "timestamp": "20240101_120000", "name": "Granite 4.0 Micro"},
    {"id": "granite-8b_v1.0", "timestamp": "20240101_130000", "name": "Granite 8B"},
]

comparison_data = []

for model in models:
    results = load_multilang_results(model["id"], model["timestamp"])
    # Extract and add to comparison_data
    # ...

# Create comparison visualization
df_comparison = pd.DataFrame(comparison_data)

fig = px.bar(
    df_comparison,
    x='model',
    y='accuracy',
    color='language',
    barmode='group',
    title='Model Comparison: Accuracy Across Languages'
)

fig.show()</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Share your evaluation dashboard with stakeholders</p>
</li>
<li>
<p>Use results to inform model selection decisions</p>
</li>
<li>
<p>Track evaluation results over time to measure model improvements</p>
</li>
<li>
<p>Integrate evaluation workflows into your CI/CD pipeline</p>
</li>
</ul>
</div>
<hr>
<div class="paragraph">
<p><strong>Evaluation complete. You now have comprehensive insights into your model&#8217;s performance.</strong></p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="domain-evaluation-lab.html">Domain-Specific Evaluation</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
