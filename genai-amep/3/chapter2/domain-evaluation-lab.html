<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab: Domain-Specific Evaluation :: Automated Multi Language LLM Evaluation Pipeline</title>
    <link rel="prev" href="performance-evaluation-lab.html">
    <link rel="next" href="results-collection.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Automated Multi Language LLM Evaluation Pipeline</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/changeme/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header><div class="body">
<div class="nav-container" data-component="genai-amep" data-version="3">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Introduction</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Platform preparation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/infrastructure-setup.html">Infrastructure setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/modelcar-pipeline.html">ModelCar Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/benchmark-pipeline.html">Benchmark Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/evaluation-pipeline.html">Evaluation Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/results-analysis.html">Results Analysis and Visualization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Model Evaluation Lab</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="multilang-evaluation-lab.html">Multi-Language Evaluation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="performance-evaluation-lab.html">Performance Benchmarking</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="domain-evaluation-lab.html">Domain-Specific Evaluation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="results-collection.html">Results Collection &amp; Visualization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Automated Multi-Language LLM Evaluation Pipeline</span>
    <span class="version">3</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">3</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a></li>
    <li><a href="index.html">Model Evaluation Lab</a></li>
    <li><a href="domain-evaluation-lab.html">Domain-Specific Evaluation</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab: Domain-Specific Evaluation</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph lead">
<p><strong>Standard benchmarks test general intelligence. Domain evaluations test business value.</strong></p>
</div>
<div class="paragraph">
<p>While standardized benchmarks (MMLU, ARC, HellaSwag) measure general model capabilities, they don&#8217;t tell you if your model understands <strong>your</strong> business domain. This lab teaches you to create and run domain-specific evaluations that measure model performance on tasks relevant to your use case.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_domain_specific_evaluation_matters"><a class="anchor" href="#_why_domain_specific_evaluation_matters"></a>Why Domain-Specific Evaluation Matters</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Domain-specific evaluation answers critical business questions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Does the model understand our terminology?</strong> (e.g., medical jargon, legal terms, financial concepts)</p>
</li>
<li>
<p><strong>Can it handle our specific use cases?</strong> (e.g., insurance claim processing, code review, customer support)</p>
</li>
<li>
<p><strong>Is it safe for our domain?</strong> (e.g., healthcare compliance, financial regulations)</p>
</li>
<li>
<p><strong>Does fine-tuning improve domain performance?</strong> (measuring ROI of customization efforts)</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before starting, ensure:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Your InferenceService is deployed and in "Ready" state</p>
</li>
<li>
<p>TrustyAI operator is installed and configured</p>
</li>
<li>
<p>You have domain-specific questions/prompts ready</p>
</li>
<li>
<p>You understand your model&#8217;s expected behavior in your domain</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_1_prepare_domain_specific_dataset"><a class="anchor" href="#_step_1_prepare_domain_specific_dataset"></a>Step 1: Prepare Domain-Specific Dataset</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First, we&#8217;ll create a custom evaluation dataset with domain-specific questions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Create evaluation directory
mkdir -p ~/domain-evaluation
cd ~/domain-evaluation

# Create a custom evaluation dataset
# Format: JSONL (one JSON object per line)
cat &gt; domain-dataset.jsonl &lt;&lt;EOF
{"input": "What is the recommended treatment for Type 2 diabetes?", "expected_output": "Metformin is typically the first-line treatment", "domain": "healthcare"}
{"input": "Explain the difference between a deductible and a copay.", "expected_output": "A deductible is the amount you pay before insurance kicks in, while a copay is a fixed amount per service", "domain": "insurance"}
{"input": "What are the key requirements for GDPR compliance?", "expected_output": "GDPR requires data protection, user consent, right to deletion, and breach notification", "domain": "legal"}
{"input": "Calculate the compound interest for $10,000 at 5% APR over 10 years.", "expected_output": "Approximately $16,288.95", "domain": "finance"}
EOF

echo "✅ Created domain dataset with $(wc -l &lt; domain-dataset.jsonl) examples"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Dataset Format</div>
<div class="paragraph">
<p>The dataset uses JSONL format (one JSON object per line). Each object should have:
* <code>input</code>: The question or prompt
* <code>expected_output</code>: The expected answer (for reference)
* <code>domain</code>: Your business domain (optional, for categorization)</p>
</div>
<div class="paragraph">
<p>You can add more fields as needed for your evaluation.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_2_create_custom_evaluation_task_definition"><a class="anchor" href="#_step_2_create_custom_evaluation_task_definition"></a>Step 2: Create Custom Evaluation Task Definition</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Create a TrustyAI-compatible task definition:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Create task definition
cat &gt; domain-task.yaml &lt;&lt;EOF
task: domain_evaluation
group: custom
description: "Domain-specific evaluation for business use cases"
metrics:
  - accuracy
  - exact_match
output_type: generate_until
EOF

echo "✅ Created task definition"</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_3_upload_dataset_to_s3"><a class="anchor" href="#_step_3_upload_dataset_to_s3"></a>Step 3: Upload Dataset to S3</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Upload your dataset to S3 so TrustyAI can access it:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Set S3 configuration
export S3_ENDPOINT="http://minio-service.s3-storage.svc.cluster.local:9000"
export S3_BUCKET="model-evaluations"
export S3_ACCESS_KEY="minio"
export S3_SECRET_KEY="minio123"

# Upload dataset
oc run aws-cli-upload --rm -i --restart=Never \
  --image=amazon/aws-cli:latest \
  --env="AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}" \
  --env="AWS_SECRET_ACCESS_KEY=${S3_SECRET_KEY}" \
  --env="AWS_DEFAULT_REGION=us-east-1" \
  -- sh -c "
    aws --endpoint-url=${S3_ENDPOINT} s3 mb s3://${S3_BUCKET} || true

    aws --endpoint-url=${S3_ENDPOINT} s3 cp \
      ~/domain-evaluation/domain-dataset.jsonl \
      s3://${S3_BUCKET}/custom-data/domain-dataset.jsonl

    aws --endpoint-url=${S3_ENDPOINT} s3 cp \
      ~/domain-evaluation/domain-task.yaml \
      s3://${S3_BUCKET}/custom-data/domain-task.yaml

    echo \"✅ Dataset uploaded to S3\"
  "</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_4_create_lmevaljob_with_custom_task"><a class="anchor" href="#_step_4_create_lmevaljob_with_custom_task"></a>Step 4: Create LMEvalJob with Custom Task</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Create an LMEvalJob that uses your custom dataset:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Set your model information
export NAMESPACE="vllm"
export MODEL_NAME="granite-4.0-micro"
export INFERENCE_URL=$(oc get inferenceservice $MODEL_NAME -n ${NAMESPACE} \
  -o jsonpath='{.status.url}')

# Create custom evaluation job
cat &gt; domain-eval-job.yaml &lt;&lt;EOF
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: domain-specific-eval
  namespace: ${NAMESPACE}
spec:
  model: local-completions
  taskList:
    taskNames:
      - domain_evaluation
  modelArgs:
    - name: model
      value: ${MODEL_NAME}
    - name: base_url
      value: ${INFERENCE_URL}/v1/completions
    - name: num_concurrent
      value: '1'
    - name: max_retries
      value: '6'
    - name: tokenized_requests
      value: 'False'
    - name: tokenizer
      value: ibm-granite/granite-4.0-micro
  logSamples: true
  batchSize: '1'
  allowOnline: true
  allowCodeExecution: false
  outputs:
    pvcManaged:
      size: 5Gi
  pod:
    volumes:
      - name: custom-dataset
        configMap:
          name: domain-dataset-cm
    container:
      volumeMounts:
        - name: custom-dataset
          mountPath: /opt/app-root/src/custom-tasks
          readOnly: true
EOF

# Create ConfigMap with custom dataset
oc create configmap domain-dataset-cm \
  --from-file=domain-dataset.jsonl=~/domain-evaluation/domain-dataset.jsonl \
  --from-file=domain-task.yaml=~/domain-evaluation/domain-task.yaml \
  -n ${NAMESPACE}

# Apply the evaluation job
oc apply -f domain-eval-job.yaml</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Custom Task Configuration</div>
<div class="paragraph">
<p>For more complex custom tasks, you may need to:
* Define custom metrics in the task YAML
* Configure task-specific parameters
* Handle different output formats</p>
</div>
<div class="paragraph">
<p>Refer to the TrustyAI documentation for advanced custom task configuration.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_5_monitor_evaluation_progress"><a class="anchor" href="#_step_5_monitor_evaluation_progress"></a>Step 5: Monitor Evaluation Progress</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Watch the evaluation job:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Watch job status
oc get lmevaljob domain-specific-eval -n ${NAMESPACE} -w</code></pre>
</div>
</div>
<div class="paragraph">
<p>Monitor the pod logs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Get pod name
POD_NAME=$(oc get pods -n ${NAMESPACE} -l job-name=domain-specific-eval \
  -o jsonpath='{.items[0].metadata.name}')

# Watch logs
oc logs -f $POD_NAME -n ${NAMESPACE}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_6_extract_and_analyze_results"><a class="anchor" href="#_step_6_extract_and_analyze_results"></a>Step 6: Extract and Analyze Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once the evaluation completes, extract the results:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Extract results (similar to multi-language evaluation)
# Get the PVC name
PVC_NAME=$(oc get lmevaljob domain-specific-eval -n ${NAMESPACE} \
  -o jsonpath='{.status.outputs.pvcManaged.name}')

# Create extraction pod
cat &gt; extract-domain-results.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: extract-domain-results
  namespace: ${NAMESPACE}
spec:
  containers:
  - name: extractor
    image: registry.access.redhat.com/ubi8/ubi-minimal:latest
    command: ["/bin/sh", "-c", "sleep 3600"]
    volumeMounts:
    - name: results
      mountPath: /results
  volumes:
  - name: results
    persistentVolumeClaim:
      claimName: ${PVC_NAME}
  restartPolicy: Never
EOF

oc apply -f extract-domain-results.yaml
oc wait --for=condition=Ready pod/extract-domain-results -n ${NAMESPACE} --timeout=60s

# Copy results
mkdir -p ~/evaluation-results/domain
oc cp ${NAMESPACE}/extract-domain-results:/results ~/evaluation-results/domain/

# Cleanup
oc delete pod extract-domain-results -n ${NAMESPACE}
rm extract-domain-results.yaml

echo "✅ Results extracted to ~/evaluation-results/domain/"</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_7_upload_results_to_s3"><a class="anchor" href="#_step_7_upload_results_to_s3"></a>Step 7: Upload Results to S3</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Upload results for later comparison:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Upload to S3
oc run aws-cli-upload --rm -i --restart=Never \
  --image=amazon/aws-cli:latest \
  --env="AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}" \
  --env="AWS_SECRET_ACCESS_KEY=${S3_SECRET_KEY}" \
  --env="AWS_DEFAULT_REGION=us-east-1" \
  -- sh -c "
    TIMESTAMP=\$(date +%Y%m%d_%H%M%S)
    MODEL_ID=\"${MODEL_NAME}_v1.0\"

    aws --endpoint-url=${S3_ENDPOINT} s3 sync \
      ~/evaluation-results/domain/ \
      s3://${S3_BUCKET}/domain/\${MODEL_ID}/\${TIMESTAMP}/

    echo \"✅ Domain evaluation results uploaded\"
  "</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_domain_specific_results"><a class="anchor" href="#_understanding_domain_specific_results"></a>Understanding Domain-Specific Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Domain evaluation results typically include:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Accuracy:</strong> Percentage of correct answers</p>
</li>
<li>
<p><strong>Exact Match:</strong> Percentage of responses that exactly match expected output</p>
</li>
<li>
<p><strong>Domain-Specific Metrics:</strong> Custom metrics relevant to your use case</p>
</li>
<li>
<p><strong>Sample Responses:</strong> Actual model outputs for manual review</p>
</li>
</ul>
</div>
<div class="sect2">
<h3 id="_interpreting_results"><a class="anchor" href="#_interpreting_results"></a>Interpreting Results</h3>
<div class="ulist">
<ul>
<li>
<p><strong>High Accuracy (&gt;80%):</strong> Model understands your domain well</p>
</li>
<li>
<p><strong>Medium Accuracy (60-80%):</strong> Model has gaps but may be usable with prompt engineering</p>
</li>
<li>
<p><strong>Low Accuracy (&lt;60%):</strong> Model may need fine-tuning or domain-specific training</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Domain Evaluation Best Practices</div>
<div class="ulist">
<ul>
<li>
<p>Start with a small, representative dataset (10-20 examples)</p>
</li>
<li>
<p>Include edge cases and common failure modes</p>
</li>
<li>
<p>Test both simple and complex queries</p>
</li>
<li>
<p>Compare results before and after fine-tuning</p>
</li>
<li>
<p>Regularly update your dataset as your domain evolves</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_example_healthcare_domain_evaluation"><a class="anchor" href="#_example_healthcare_domain_evaluation"></a>Example: Healthcare Domain Evaluation</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Here&#8217;s an example for healthcare domain:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Healthcare-specific dataset
cat &gt; healthcare-dataset.jsonl &lt;&lt;EOF
{"input": "What are the symptoms of Type 2 diabetes?", "expected_output": "Increased thirst, frequent urination, fatigue, blurred vision", "domain": "healthcare", "category": "symptoms"}
{"input": "What is the normal blood pressure range?", "expected_output": "Normal blood pressure is typically below 120/80 mmHg", "domain": "healthcare", "category": "vitals"}
{"input": "Explain the mechanism of action of metformin.", "expected_output": "Metformin reduces glucose production in the liver and improves insulin sensitivity", "domain": "healthcare", "category": "medications"}
EOF</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Proceed to <a href="results-collection.html" class="xref page">Results Collection &amp; Visualization</a> to analyze all your evaluation results together</p>
</li>
<li>
<p>Compare domain-specific results with general benchmarks to identify gaps</p>
</li>
<li>
<p>Use results to inform fine-tuning or prompt engineering efforts</p>
</li>
</ul>
</div>
<hr>
<div class="paragraph">
<p><strong>Domain knowledge tested. Now let&#8217;s visualize all the results.</strong></p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="performance-evaluation-lab.html">Performance Benchmarking</a></span>
  <span class="next"><a href="results-collection.html">Results Collection &amp; Visualization</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
