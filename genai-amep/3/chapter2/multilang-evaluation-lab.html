<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Lab: Multi-Language Accuracy Evaluation :: Automated Multi Language LLM Evaluation Pipeline</title>
    <link rel="prev" href="index.html">
    <link rel="next" href="performance-evaluation-lab.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Automated Multi Language LLM Evaluation Pipeline</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/changeme/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header><div class="body">
<div class="nav-container" data-component="genai-amep" data-version="3">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Introduction</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter1/index.html">Platform preparation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/infrastructure-setup.html">Infrastructure setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/modelcar-pipeline.html">ModelCar Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/benchmark-pipeline.html">Benchmark Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/evaluation-pipeline.html">Evaluation Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter1/results-analysis.html">Results Analysis and Visualization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Model Evaluation Lab</a>
<ul class="nav-list">
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="multilang-evaluation-lab.html">Multi-Language Evaluation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="performance-evaluation-lab.html">Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="domain-evaluation-lab.html">Domain-Specific Evaluation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="results-collection.html">Results Collection &amp; Visualization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Automated Multi-Language LLM Evaluation Pipeline</span>
    <span class="version">3</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">3</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a></li>
    <li><a href="index.html">Model Evaluation Lab</a></li>
    <li><a href="multilang-evaluation-lab.html">Multi-Language Evaluation</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Lab: Multi-Language Accuracy Evaluation</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph lead">
<p><strong>Accuracy is not universal. A model that excels in English may struggle in Spanish or Japanese.</strong></p>
</div>
<div class="paragraph">
<p>In this lab, you will evaluate your deployed model&#8217;s accuracy across multiple languages using standardized benchmarks. This evaluation answers the critical question: <strong>"How well does my model perform across different languages?"</strong></p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_why_multi_language_evaluation_matters"><a class="anchor" href="#_why_multi_language_evaluation_matters"></a>Why Multi-Language Evaluation Matters</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Enterprise AI systems must serve global audiences. A model that performs well in English but fails in Spanish or Japanese creates:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>User Experience Gaps:</strong> Inconsistent quality across languages damages user trust</p>
</li>
<li>
<p><strong>Business Risk:</strong> Poor performance in key markets can impact revenue</p>
</li>
<li>
<p><strong>Deployment Blind Spots:</strong> Without multi-language testing, you may deploy models that fail in production</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>This lab uses <strong>TrustyAI LMEvalJob</strong> to run standardized accuracy benchmarks across three languages:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>English (en):</strong> ARC-Easy, HellaSwag, WinoGrande, TruthfulQA</p>
</li>
<li>
<p><strong>Spanish (es):</strong> BELEBELE Spanish, XNLI Spanish</p>
</li>
<li>
<p><strong>Japanese (ja):</strong> BELEBELE Japanese, XNLI Japanese</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before starting, ensure:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Your InferenceService is deployed and in "Ready" state</p>
</li>
<li>
<p>TrustyAI operator is installed and configured (from llmops-lmeval course)</p>
</li>
<li>
<p>You know your model&#8217;s inference endpoint URL</p>
</li>
<li>
<p>You have access to create LMEvalJob resources in your namespace</p>
</li>
</ul>
</div>
<div class="admonitionblock important">
<table>
<tr>
<td class="icon">
<i class="fa icon-important" title="Important"></i>
</td>
<td class="content">
<div class="title">Verify TrustyAI Configuration</div>
<div class="paragraph">
<p>Ensure TrustyAI is configured for remote dataset access:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Check TrustyAI ConfigMap
oc get configmap trustyai-service-operator-config -n redhat-ods-applications -o yaml | grep -A 5 "lmes-allow"

# If not configured, enable online access:
oc patch configmap trustyai-service-operator-config -n redhat-ods-applications \
  --type merge -p '{"data":{"lmes-allow-online":"true","lmes-allow-code-execution":"true"}}'

# Restart the operator
oc rollout restart deployment trustyai-service-operator-controller-manager -n redhat-ods-applications</code></pre>
</div>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_1_identify_your_model_endpoint"><a class="anchor" href="#_step_1_identify_your_model_endpoint"></a>Step 1: Identify Your Model Endpoint</h2>
<div class="sectionbody">
<div class="paragraph">
<p>First, we need to determine your model&#8217;s inference endpoint URL.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Set your namespace (adjust as needed)
export NAMESPACE="vllm"  # or your model namespace
export MODEL_NAME="granite-4.0-micro"  # your model name

# Get the inference endpoint
oc get inferenceservice $MODEL_NAME -n $NAMESPACE \
  -o jsonpath='{.status.url}'

# Save it for later use
export INFERENCE_URL=$(oc get inferenceservice $MODEL_NAME -n $NAMESPACE \
  -o jsonpath='{.status.url}')
echo "Inference URL: $INFERENCE_URL"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Endpoint Format</div>
<div class="paragraph">
<p>The endpoint URL should look like:
<code><a href="https://&lt;model-name&gt;-predictor-&lt;namespace&gt;.&lt;cluster-domain&gt;/v1" class="bare">https://&lt;model-name&gt;-predictor-&lt;namespace&gt;.&lt;cluster-domain&gt;/v1</a></code></p>
</div>
<div class="paragraph">
<p>For completions endpoint, append <code>/completions</code> or <code>/chat/completions</code> depending on your model type.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_2_create_evaluation_job_definitions"><a class="anchor" href="#_step_2_create_evaluation_job_definitions"></a>Step 2: Create Evaluation Job Definitions</h2>
<div class="sectionbody">
<div class="paragraph">
<p>We&#8217;ll create LMEvalJob resources for each language. Start by creating a directory for your evaluation configurations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Create evaluation directory
mkdir -p ~/model-evaluations/multilang
cd ~/model-evaluations/multilang</code></pre>
</div>
</div>
<div class="sect2">
<h3 id="_2_1_english_evaluation_job"><a class="anchor" href="#_2_1_english_evaluation_job"></a>2.1 English Evaluation Job</h3>
<div class="paragraph">
<p>Create the English evaluation job:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cat &gt; english-eval.yaml &lt;&lt;EOF
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: multilang-english-eval
  namespace: ${NAMESPACE}
spec:
  model: local-completions
  taskList:
    taskNames:
      - arc_easy
      - hellaswag
      - winogrande
      - truthfulqa_mc2
  modelArgs:
    - name: model
      value: ${MODEL_NAME}
    - name: base_url
      value: ${INFERENCE_URL}/v1/completions
    - name: num_concurrent
      value: '1'
    - name: max_retries
      value: '6'
    - name: tokenized_requests
      value: 'False'
    - name: tokenizer
      value: ibm-granite/granite-4.0-micro
  logSamples: true
  batchSize: '1'
  allowOnline: true
  allowCodeExecution: false
  outputs:
    pvcManaged:
      size: 5Gi
EOF</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_2_2_spanish_evaluation_job"><a class="anchor" href="#_2_2_spanish_evaluation_job"></a>2.2 Spanish Evaluation Job</h3>
<div class="paragraph">
<p>Create the Spanish evaluation job:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cat &gt; spanish-eval.yaml &lt;&lt;EOF
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: multilang-spanish-eval
  namespace: ${NAMESPACE}
spec:
  model: local-completions
  taskList:
    taskNames:
      - belebele_spa_Latn
      - xnli_es
  modelArgs:
    - name: model
      value: ${MODEL_NAME}
    - name: base_url
      value: ${INFERENCE_URL}/v1/completions
    - name: num_concurrent
      value: '1'
    - name: max_retries
      value: '6'
    - name: tokenized_requests
      value: 'False'
    - name: tokenizer
      value: ibm-granite/granite-4.0-micro
  logSamples: true
  batchSize: '1'
  allowOnline: true
  allowCodeExecution: false
  outputs:
    pvcManaged:
      size: 5Gi
EOF</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_2_3_japanese_evaluation_job"><a class="anchor" href="#_2_3_japanese_evaluation_job"></a>2.3 Japanese Evaluation Job</h3>
<div class="paragraph">
<p>Create the Japanese evaluation job:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cat &gt; japanese-eval.yaml &lt;&lt;EOF
apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: multilang-japanese-eval
  namespace: ${NAMESPACE}
spec:
  model: local-completions
  taskList:
    taskNames:
      - belebele_jpn_Jpan
      - xnli_ja
  modelArgs:
    - name: model
      value: ${MODEL_NAME}
    - name: base_url
      value: ${INFERENCE_URL}/v1/completions
    - name: num_concurrent
      value: '1'
    - name: max_retries
      value: '6'
    - name: tokenized_requests
      value: 'False'
    - name: tokenizer
      value: ibm-granite/granite-4.0-micro
  logSamples: true
  batchSize: '1'
  allowOnline: true
  allowCodeExecution: false
  outputs:
    pvcManaged:
      size: 5Gi
EOF</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Tokenizer Configuration</div>
<div class="paragraph">
<p>Update the <code>tokenizer</code> value in each job to match your model. For IBM Granite models, use the Hugging Face model path (e.g., <code>ibm-granite/granite-4.0-micro</code>).</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_3_run_the_evaluations"><a class="anchor" href="#_step_3_run_the_evaluations"></a>Step 3: Run the Evaluations</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now we&#8217;ll run each evaluation job sequentially. Each job will take 10-30 minutes depending on your model size and hardware.</p>
</div>
<div class="sect2">
<h3 id="_3_1_run_english_evaluation"><a class="anchor" href="#_3_1_run_english_evaluation"></a>3.1 Run English Evaluation</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Apply the English evaluation job
oc apply -f english-eval.yaml

# Watch the job progress
oc get lmevaljob multilang-english-eval -n ${NAMESPACE} -w</code></pre>
</div>
</div>
<div class="paragraph">
<p>Monitor the pod logs to see progress:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Get the pod name
POD_NAME=$(oc get pods -n ${NAMESPACE} -l job-name=multilang-english-eval -o jsonpath='{.items[0].metadata.name}')

# Watch logs
oc logs -f $POD_NAME -n ${NAMESPACE}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Wait for the job to complete (status will show "Succeeded").</p>
</div>
</div>
<div class="sect2">
<h3 id="_3_2_run_spanish_evaluation"><a class="anchor" href="#_3_2_run_spanish_evaluation"></a>3.2 Run Spanish Evaluation</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Apply the Spanish evaluation job
oc apply -f spanish-eval.yaml

# Watch progress
oc get lmevaljob multilang-spanish-eval -n ${NAMESPACE} -w</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_3_3_run_japanese_evaluation"><a class="anchor" href="#_3_3_run_japanese_evaluation"></a>3.3 Run Japanese Evaluation</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Apply the Japanese evaluation job
oc apply -f japanese-eval.yaml

# Watch progress
oc get lmevaljob multilang-japanese-eval -n ${NAMESPACE} -w</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Running Evaluations in Parallel</div>
<div class="paragraph">
<p>You can run all three evaluations in parallel if you have sufficient GPU resources. However, running them sequentially is safer and easier to monitor.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_4_extract_results"><a class="anchor" href="#_step_4_extract_results"></a>Step 4: Extract Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Once all evaluations complete, extract the results from the PVCs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Create results directory
mkdir -p ~/evaluation-results/multilang
cd ~/evaluation-results/multilang

# Function to extract results from a job
extract_results() {
  local JOB_NAME=$1
  local LANG=$2

  # Get the PVC name
  PVC_NAME=$(oc get lmevaljob $JOB_NAME -n ${NAMESPACE} \
    -o jsonpath='{.status.outputs.pvcManaged.name}')

  if [ -z "$PVC_NAME" ]; then
    echo "‚ö†Ô∏è  No PVC found for $JOB_NAME"
    return
  fi

  # Create a pod to access the PVC
  cat &gt; extract-pod.yaml &lt;&lt;EOF
apiVersion: v1
kind: Pod
metadata:
  name: extract-${LANG}-results
  namespace: ${NAMESPACE}
spec:
  containers:
  - name: extractor
    image: registry.access.redhat.com/ubi8/ubi-minimal:latest
    command: ["/bin/sh", "-c", "sleep 3600"]
    volumeMounts:
    - name: results
      mountPath: /results
  volumes:
  - name: results
    persistentVolumeClaim:
      claimName: ${PVC_NAME}
  restartPolicy: Never
EOF

  oc apply -f extract-pod.yaml

  # Wait for pod to be ready
  oc wait --for=condition=Ready pod/extract-${LANG}-results -n ${NAMESPACE} --timeout=60s

  # Copy results
  oc cp ${NAMESPACE}/extract-${LANG}-results:/results ./${LANG}-results

  # Cleanup
  oc delete pod extract-${LANG}-results -n ${NAMESPACE}
  rm extract-pod.yaml

  echo "‚úÖ Extracted ${LANG} results to ./${LANG}-results/"
}

# Extract results for each language
extract_results multilang-english-eval en
extract_results multilang-spanish-eval es
extract_results multilang-japanese-eval ja</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_5_upload_results_to_s3"><a class="anchor" href="#_step_5_upload_results_to_s3"></a>Step 5: Upload Results to S3</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Now we&#8217;ll upload the results to S3 for later analysis and comparison:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Set S3 configuration (adjust to your environment)
export S3_ENDPOINT="http://minio-service.s3-storage.svc.cluster.local:9000"
export S3_BUCKET="model-evaluations"
export S3_ACCESS_KEY="minio"  # Update with your credentials
export S3_SECRET_KEY="minio123"  # Update with your credentials

# Install AWS CLI if needed
# For OpenShift, use a pod with AWS CLI
oc run aws-cli --rm -i --restart=Never \
  --image=amazon/aws-cli:latest \
  --env="AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}" \
  --env="AWS_SECRET_ACCESS_KEY=${S3_SECRET_KEY}" \
  --env="AWS_DEFAULT_REGION=us-east-1" \
  -- sh -c "
    aws --endpoint-url=${S3_ENDPOINT} s3 mb s3://${S3_BUCKET} || true

    # Create timestamped directory
    TIMESTAMP=\$(date +%Y%m%d_%H%M%S)
    MODEL_ID=\"${MODEL_NAME}_v1.0\"

    # Upload results
    for lang in en es ja; do
      if [ -d ~/evaluation-results/multilang/\${lang}-results ]; then
        aws --endpoint-url=${S3_ENDPOINT} s3 sync \
          ~/evaluation-results/multilang/\${lang}-results \
          s3://${S3_BUCKET}/multilang/\${MODEL_ID}/\${TIMESTAMP}/\${lang}/
        echo \"‚úÖ Uploaded \${lang} results\"
      fi
    done

    echo \"üìä Results uploaded to: s3://${S3_BUCKET}/multilang/\${MODEL_ID}/\${TIMESTAMP}/\"
  "</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="title">Using Data Connections</div>
<div class="paragraph">
<p>If you&#8217;re using OpenShift AI Data Connections, you can mount the S3 connection directly to a pod instead of using AWS CLI credentials.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_step_6_verify_results"><a class="anchor" href="#_step_6_verify_results"></a>Step 6: Verify Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Check that your results were uploaded successfully:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># List uploaded files
oc run aws-cli-verify --rm -i --restart=Never \
  --image=amazon/aws-cli:latest \
  --env="AWS_ACCESS_KEY_ID=${S3_ACCESS_KEY}" \
  --env="AWS_SECRET_ACCESS_KEY=${S3_SECRET_KEY}" \
  --env="AWS_DEFAULT_REGION=us-east-1" \
  -- sh -c "
    aws --endpoint-url=${S3_ENDPOINT} s3 ls \
      s3://${S3_BUCKET}/multilang/ --recursive
  "</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_the_results"><a class="anchor" href="#_understanding_the_results"></a>Understanding the Results</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Each language evaluation produces accuracy metrics for the benchmarks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>ARC-Easy:</strong> Grade-school science questions (accuracy)</p>
</li>
<li>
<p><strong>HellaSwag:</strong> Commonsense reasoning (normalized accuracy)</p>
</li>
<li>
<p><strong>WinoGrande:</strong> Pronoun resolution (accuracy)</p>
</li>
<li>
<p><strong>TruthfulQA:</strong> Truthfulness detection (multiple-choice accuracy)</p>
</li>
<li>
<p><strong>BELEBELE:</strong> Reading comprehension (accuracy)</p>
</li>
<li>
<p><strong>XNLI:</strong> Cross-lingual natural language inference (accuracy)</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Typical performance ranges:
* <strong>English:</strong> 0.60-0.80 accuracy (depending on model size)
* <strong>Spanish:</strong> 0.55-0.70 accuracy (5-10% lower than English)
* <strong>Japanese:</strong> 0.50-0.65 accuracy (10-15% lower than English)</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Proceed to <a href="performance-evaluation-lab.html" class="xref page">Performance Benchmarking</a> to measure system performance</p>
</li>
<li>
<p>Or continue to <a href="domain-evaluation-lab.html" class="xref page">Domain-Specific Evaluation</a> for business-focused testing</p>
</li>
<li>
<p>View <a href="results-collection.html" class="xref page">Results Collection &amp; Visualization</a> to analyze and compare your results</p>
</li>
</ul>
</div>
<hr>
<div class="paragraph">
<p><strong>Accuracy evaluated. Now let&#8217;s measure performance.</strong></p>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="index.html">Model Evaluation Lab</a></span>
  <span class="next"><a href="performance-evaluation-lab.html">Performance Benchmarking</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
