<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>Evaluation Pipeline Deployment :: Automated Multi Language LLM Evaluation Pipeline</title>
    <link rel="prev" href="benchmark-pipeline.html">
    <link rel="next" href="results-analysis.html">
    <meta name="generator" content="Antora 3.1.3">
    <link rel="stylesheet" href="../../../_/css/site.css">
    <script>var uiRootPath = '../../../_'</script>
  </head>
  <body class="article">
<header class="header">
  <nav class="navbar">
    <div class="navbar-brand">
      <a class="navbar-item" href="https://www.redhat.com" target="_blank"><img src="../../../_/img/redhat-logo.png" height="40px" alt="Red Hat"></a>
      <a class="navbar-item" style="font-size: 24px; color: white" href="../../..">Automated Multi Language LLM Evaluation Pipeline</a>
      <button class="navbar-burger" data-target="topbar-nav">
        <span></span>
        <span></span>
        <span></span>
      </button>
    </div>
    <div id="topbar-nav" class="navbar-menu">
      <div class="navbar-end">
        <a class="navbar-item" href="https://github.com/RedHatQuickCourses/changeme/issues" target="_blank">Report Issues</a>
      </div>
    </div>
  </nav>
</header><div class="body">
<div class="nav-container" data-component="genai-amep" data-version="3">
  <aside class="nav">
    <div class="panels">
<div class="nav-panel-menu is-active" data-panel="menu">
  <nav class="nav-menu">
    <h3 class="title"><a href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a></h3>
<ul class="nav-list">
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <a class="nav-link" href="../index.html">Introduction</a>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="index.html">Platform preparation</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="infrastructure-setup.html">Infrastructure setup</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="modelcar-pipeline.html">ModelCar Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="benchmark-pipeline.html">Benchmark Pipeline Deployment</a>
  </li>
  <li class="nav-item is-current-page" data-depth="2">
    <a class="nav-link" href="evaluation-pipeline.html">Evaluation Pipeline Deployment</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="results-analysis.html">Results Analysis and Visualization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
  <li class="nav-item" data-depth="0">
<ul class="nav-list">
  <li class="nav-item" data-depth="1">
    <button class="nav-item-toggle"></button>
    <a class="nav-link" href="../chapter2/index.html">Model Evaluation Lab</a>
<ul class="nav-list">
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/multilang-evaluation-lab.html">Multi-Language Evaluation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/performance-evaluation-lab.html">Performance Benchmarking</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/domain-evaluation-lab.html">Domain-Specific Evaluation</a>
  </li>
  <li class="nav-item" data-depth="2">
    <a class="nav-link" href="../chapter2/results-collection.html">Results Collection &amp; Visualization</a>
  </li>
</ul>
  </li>
</ul>
  </li>
</ul>
  </nav>
</div>
<div class="nav-panel-explore" data-panel="explore">
  <div class="context">
    <span class="title">Automated Multi-Language LLM Evaluation Pipeline</span>
    <span class="version">3</span>
  </div>
  <ul class="components">
    <li class="component is-current">
      <a class="title" href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a>
      <ul class="versions">
        <li class="version is-current is-latest">
          <a href="../index.html">3</a>
        </li>
      </ul>
    </li>
  </ul>
</div>
    </div>
  </aside>
</div>
<main class="article">
<div class="toolbar" role="navigation">
<button class="nav-toggle"></button>
  <a href="../index.html" class="home-link"></a>
<nav class="breadcrumbs" aria-label="breadcrumbs">
  <ul>
    <li><a href="../index.html">Automated Multi-Language LLM Evaluation Pipeline</a></li>
    <li><a href="index.html">Platform preparation</a></li>
    <li><a href="evaluation-pipeline.html">Evaluation Pipeline Deployment</a></li>
  </ul>
</nav>
</div>
  <div class="content">
<aside class="toc sidebar" data-title="Contents" data-levels="2">
  <div class="toc-menu"></div>
</aside>
<article class="doc">
<h1 class="page">Evaluation Pipeline Deployment</h1>
<div id="preamble">
<div class="sectionbody">
<div class="paragraph">
<p>This guide covers the deployment and usage of the GuideLLM evaluation pipeline for comprehensive model assessment on OpenShift AI.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_overview"><a class="anchor" href="#_overview"></a>Overview</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The evaluation pipeline provides automated testing and benchmarking of deployed models. It works in conjunction with the <a href="modelcar-pipeline.html" class="xref page">ModelCar Pipeline</a> to evaluate models that have been packaged and optionally deployed.</p>
</div>
<div class="paragraph">
<p>The evaluation pipeline consists of multiple integrated evaluation tasks:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><strong>Model Deployment</strong> (optional): Deploys models using Helm if not already deployed</p>
</li>
<li>
<p><strong>GuideLLM Benchmarking</strong>: Performance, throughput, and latency testing</p>
</li>
<li>
<p><strong>LM-Eval Tasks</strong>: Accuracy evaluation using TrustyAI LMEvalJob (MMLU, HumanEval, MBPP, etc.)</p>
</li>
<li>
<p><strong>Custom Evaluation Tasks</strong>: Support for custom evaluation datasets</p>
</li>
<li>
<p><strong>S3 Storage</strong>: Automated upload of results to object storage</p>
</li>
<li>
<p><strong>Model Registry Integration</strong>: Automatic registration of evaluation results</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Relationship to ModelCar Pipeline:</strong>
* The ModelCar pipeline packages models into OCI images and can deploy them
* The evaluation pipeline evaluates already-deployed models OR can deploy them first
* Both pipelines can work independently, but typically you:
  1. Package a model using ModelCar pipeline
  2. Evaluate the deployed model using this evaluation pipeline
  3. Results are stored in S3 and registered in the Model Registry</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_prerequisites"><a class="anchor" href="#_prerequisites"></a>Prerequisites</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Before deploying the evaluation pipeline, ensure you have:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="infrastructure-setup.html" class="xref page">Infrastructure Setup</a> completed (MinIO, Model Registry)</p>
</li>
<li>
<p><a href="modelcar-pipeline.html" class="xref page">ModelCar Pipeline</a> deployed and a model packaged</p>
</li>
<li>
<p>OpenShift AI 3.0+ with TrustyAI operator installed (for LMEvalJob support)</p>
</li>
<li>
<p>A deployed model (InferenceService) to evaluate, or the pipeline will deploy one</p>
</li>
<li>
<p>Pipeline namespace configured (e.g., <code>modelcar-pipelines</code>)</p>
</li>
<li>
<p>A separate namespace for model deployments (e.g., <code>vllm</code>) - this is where LMEvalJobs run</p>
</li>
<li>
<p>ServiceAccount named <code>pipeline</code> in the pipeline namespace with appropriate permissions</p>
</li>
</ul>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_pipeline_architecture"><a class="anchor" href="#_pipeline_architecture"></a>Pipeline Architecture</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The evaluation pipeline orchestrates the following workflow:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-mermaid hljs" data-lang="mermaid">graph LR
    A[Deploy Model] --&gt; B[GuideLLM Benchmark]
    B --&gt; C[Upload GuideLLM Results]
    C --&gt; D[LM-Eval Tasks]
    D --&gt; E[Upload LM-Eval Results]
    E --&gt; F[Register Results in Model Registry]</code></pre>
</div>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p><strong>Deploy Model</strong>: Optionally deploys the model as a vLLM InferenceService using Helm (if not already deployed)</p>
</li>
<li>
<p><strong>GuideLLM Benchmark</strong>: Runs performance benchmarking against the model endpoint</p>
</li>
<li>
<p><strong>Upload GuideLLM Results</strong>: Stores benchmark results in S3 (bucket: <code>guidellm-results</code>)</p>
</li>
<li>
<p><strong>LM-Eval Tasks</strong>: Creates and runs TrustyAI LMEvalJob for accuracy evaluation</p>
</li>
<li>
<p><strong>Upload LM-Eval Results</strong>: Stores evaluation results in S3 (bucket: <code>lm-eval-results</code>)</p>
</li>
<li>
<p><strong>Register Results</strong>: Updates model registry with evaluation metadata and links to results</p>
</li>
</ol>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_deploy_pipeline_components"><a class="anchor" href="#_deploy_pipeline_components"></a>Deploy Pipeline Components</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_create_pipeline_storage"><a class="anchor" href="#_create_pipeline_storage"></a>Create Pipeline Storage</h3>
<div class="paragraph">
<p>Create a PVC for storing evaluation results:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f deploy/guidellm-pipeline/pipeline/pvc.yaml -n modelcar-pipelines</code></pre>
</div>
</div>
<div class="paragraph">
<p>This creates <code>guidellm-output-pvc</code> for workspace storage.</p>
</div>
</div>
<div class="sect2">
<h3 id="_deploy_evaluation_tasks"><a class="anchor" href="#_deploy_evaluation_tasks"></a>Deploy Evaluation Tasks</h3>
<div class="paragraph">
<p>Deploy the individual Tekton tasks:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Deploy model task
oc apply -f deploy/guidellm-pipeline/pipeline/deploy-model-task.yaml -n modelcar-pipelines

# GuideLLM benchmark task
oc apply -f deploy/guidellm-pipeline/pipeline/guidellm-benchmark-task.yaml -n modelcar-pipelines

# Upload GuideLLM results task
oc apply -f deploy/guidellm-pipeline/pipeline/upload-guidellm-results-task.yaml -n modelcar-pipelines

# LM-Eval task
oc apply -f deploy/guidellm-pipeline/pipeline/lm-eval-task.yaml -n modelcar-pipelines

# Upload LM-Eval results task
oc apply -f deploy/guidellm-pipeline/pipeline/upload-lm-eval-results-task.yaml -n modelcar-pipelines

# Model registry integration task
oc apply -f deploy/guidellm-pipeline/pipeline/model-registry-task.yaml -n modelcar-pipelines</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_create_evaluation_configmaps"><a class="anchor" href="#_create_evaluation_configmaps"></a>Create Evaluation ConfigMaps</h3>
<div class="paragraph">
<p>The pipeline requires ConfigMaps for LM-Eval job definitions. Note that <code>mmlu.yaml</code> is actually a LMEvalJob Custom Resource, not a ConfigMap:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Create namespace for model deployments (if it doesn't exist)
oc create namespace vllm --dry-run=client -o yaml | oc apply -f -

# Create ConfigMap for standard MMLU evaluation job manifest
oc create configmap mmlu-manifest \
  --from-file=mmlu.yaml=deploy/guidellm-pipeline/pipeline/mmlu.yaml \
  -n modelcar-pipelines

# Create ConfigMap for custom evaluation (if using custom datasets)
oc create configmap custom-mmlu \
  --from-file=custom-mmlu.yaml=deploy/guidellm-pipeline/custom-lm-eval/custom-mmlu.yaml \
  --from-file=task.yaml=deploy/guidellm-pipeline/custom-lm-eval/task.yaml \
  --from-file=custom-task.yaml=deploy/guidellm-pipeline/custom-lm-eval/custom-task.yaml \
  --from-file=data.jsonl=deploy/guidellm-pipeline/custom-lm-eval/data.jsonl \
  -n modelcar-pipelines

# For custom evaluations, you may also need to create a ConfigMap with your custom data
# oc create configmap custom-lmeval-benchmark-files \
#   --from-file=task.yaml=deploy/guidellm-pipeline/custom-lm-eval/task.yaml \
#   --from-file=data.jsonl=deploy/guidellm-pipeline/custom-lm-eval/data.jsonl \
#   -n vllm</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The LMEvalJob Custom Resources are applied to the <code>vllm</code> namespace (or your model deployment namespace), not the pipeline namespace. The ConfigMaps are used by the pipeline to store the job definitions.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_deploy_main_pipeline"><a class="anchor" href="#_deploy_main_pipeline"></a>Deploy Main Pipeline</h3>
<div class="paragraph">
<p>Deploy the main evaluation pipeline:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc apply -f deploy/guidellm-pipeline/pipeline/benchmark-eval-pipeline.yaml -n modelcar-pipelines</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_create_serviceaccount"><a class="anchor" href="#_create_serviceaccount"></a>Create ServiceAccount</h3>
<div class="paragraph">
<p>The pipeline requires a ServiceAccount with permissions to create and manage resources:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Create ServiceAccount (if not already exists)
oc create serviceaccount pipeline -n modelcar-pipelines --dry-run=client -o yaml | oc apply -f -

# Grant necessary permissions (adjust as needed for your environment)
# The ServiceAccount needs permissions to:
# - Create/delete LMEvalJobs in the vllm namespace
# - Access ConfigMaps and PVCs
# - Deploy models using Helm (if using deploy-model task)</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_verify_pipeline_deployment"><a class="anchor" href="#_verify_pipeline_deployment"></a>Verify Pipeline Deployment</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Check tasks
oc get tasks -n modelcar-pipelines | grep -E "(guidellm-benchmark|lm-eval|deploy-model|upload-guidellm|upload-lm-eval|model-registry)"

# Check pipeline
oc get pipeline guidellm-benchmark-pipeline -n modelcar-pipelines

# Check ConfigMaps
oc get configmap -n modelcar-pipelines | grep -E "(mmlu-manifest|custom-mmlu)"

# Check ServiceAccount
oc get serviceaccount pipeline -n modelcar-pipelines</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_running_the_evaluation_pipeline"><a class="anchor" href="#_running_the_evaluation_pipeline"></a>Running the Evaluation Pipeline</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_basic_evaluation_run"><a class="anchor" href="#_basic_evaluation_run"></a>Basic Evaluation Run</h3>
<div class="paragraph">
<p>To evaluate a deployed model:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">oc create -f - &lt;&lt;EOF
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: guidellm-benchmark-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: guidellm-benchmark-pipeline
  params:
    - name: target
      value: "http://granite-2b-predictor.vllm.svc.cluster.local:8080/v1"
    - name: model-name
      value: "granite-2b"
    - name: processor
      value: "ibm-granite/granite-3.3-2b-instruct"
    - name: data-config
      value: "prompt_tokens=800,output_tokens=128"
    - name: max-seconds
      value: "30"
    - name: rate-type
      value: "sweep"
    - name: rate
      value: "1.0, 4.0, 8.0, 16.0"
    - name: api-key
      value: ""
    - name: max-concurrency
      value: "10"
    - name: huggingface-token
      value: ""
    - name: s3-api-endpoint
      value: "http://minio-service.s3-storage.svc.cluster.local:9000"
    - name: s3-access-key-id
      value: "minio"
    - name: s3-secret-access-key
      value: "minio123"
    - name: model-url
      value: "quay.io/&lt;YOUR_ORG&gt;/granite-3.3-2b-instruct:latest"
    - name: lm-eval-job-name
      value: "mmlu-jurisprudence-eval-job"
    - name: lm-eval-custom
      value: "False"
    - name: custom-data
      value: "False"
    - name: custom-filename
      value: "no-file"
    - name: model-reg-author
      value: "ModelOps Team"
    - name: valuesContent
      value: |
        deploymentMode: RawDeployment
        fullnameOverride: "granite-2b"
        model:
          modelNameOverride: "granite-2b"
          uri: "oci://quay.io/&lt;YOUR_ORG&gt;/granite-3.3-2b-instruct:latest"
          args:
            - "--disable-log-requests"
            - "--max-num-seqs=32"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: guidellm-output-pvc
    - name: manifests
      configMap:
        name: mmlu-manifest
    - name: custom-mmlu
      configMap:
        name: custom-mmlu
  taskRunTemplate:
    serviceAccountName: pipeline
  timeouts:
    pipeline: 1h0m0s
EOF</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Important Notes:</strong>
* The <code>target</code> parameter should point to your deployed model&#8217;s endpoint. If the model is not yet deployed, the <code>deploy-model</code> task will deploy it using the Helm chart specified in <code>valuesContent</code>.
* The <code>lm-eval-job-name</code> must match the name in the LMEvalJob CR (e.g., <code>mmlu-jurisprudence-eval-job</code> from <code>mmlu.yaml</code>).
* The LMEvalJob will be created in the <code>vllm</code> namespace (or the namespace specified in the job definition).
* Ensure the model endpoint URL format matches: <code><a href="http://&lt;SERVICE_NAME&gt;-predictor.&lt;NAMESPACE&gt;.svc.cluster.local:8080/v1" class="bare">http://&lt;SERVICE_NAME&gt;-predictor.&lt;NAMESPACE&gt;.svc.cluster.local:8080/v1</a></code></p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="_custom_evaluation_run"><a class="anchor" href="#_custom_evaluation_run"></a>Custom Evaluation Run</h3>
<div class="paragraph">
<p>To run custom evaluation tasks with custom data:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># First, upload your custom data file to the S3 'custom-data' bucket
# Then create the PipelineRun:

oc create -f - &lt;&lt;EOF
apiVersion: tekton.dev/v1
kind: PipelineRun
metadata:
  generateName: custom-eval-run-
  namespace: modelcar-pipelines
spec:
  pipelineRef:
    name: guidellm-benchmark-pipeline
  params:
    - name: target
      value: "http://my-model-predictor.vllm.svc.cluster.local:8080/v1"
    - name: model-name
      value: "my-custom-model"
    - name: processor
      value: "org/model-repo"
    - name: data-config
      value: "prompt_tokens=1024,output_tokens=256"
    - name: max-seconds
      value: "60"
    - name: rate-type
      value: "fixed"
    - name: rate
      value: "10.0"
    - name: api-key
      value: ""
    - name: max-concurrency
      value: "10"
    - name: huggingface-token
      value: ""
    - name: s3-api-endpoint
      value: "http://minio-service.s3-storage.svc.cluster.local:9000"
    - name: s3-access-key-id
      value: "minio"
    - name: s3-secret-access-key
      value: "minio123"
    - name: model-url
      value: "quay.io/&lt;YOUR_ORG&gt;/my-custom-model:latest"
    - name: lm-eval-job-name
      value: "custom-eval-job"
    - name: lm-eval-custom
      value: "True"
    - name: custom-data
      value: "True"
    - name: custom-filename
      value: "my-custom-prompts.csv"
    - name: model-reg-author
      value: "ModelOps Team"
    - name: valuesContent
      value: |
        deploymentMode: RawDeployment
        fullnameOverride: "my-custom-model"
        model:
          modelNameOverride: "my-custom-model"
          uri: "oci://quay.io/&lt;YOUR_ORG&gt;/my-custom-model:latest"
  workspaces:
    - name: shared-workspace
      persistentVolumeClaim:
        claimName: guidellm-output-pvc
    - name: manifests
      configMap:
        name: mmlu-manifest
    - name: custom-mmlu
      configMap:
        name: custom-mmlu
  taskRunTemplate:
    serviceAccountName: pipeline
  timeouts:
    pipeline: 2h0m0s
EOF</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p><strong>Custom Data Setup:</strong>
* Upload your custom prompt file (CSV, JSONL, etc.) to the S3 bucket named <code>custom-data</code> in MinIO
* Set <code>custom-data: "True"</code> and provide the filename in <code>custom-filename</code>
* The file will be downloaded from S3 during the benchmark task
* For custom LM-Eval tasks, ensure your <code>custom-mmlu</code> ConfigMap contains the task definition and data files</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_pipeline_parameters_reference"><a class="anchor" href="#_pipeline_parameters_reference"></a>Pipeline Parameters Reference</h2>
<div class="sectionbody">
<table class="tableblock frame-all grid-all stretch">
<colgroup>
<col style="width: 25%;">
<col style="width: 50%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Parameter</th>
<th class="tableblock halign-left valign-top">Description</th>
<th class="tableblock halign-left valign-top">Default</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>target</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model endpoint URL (e.g., <code><a href="http://service.namespace.svc.cluster.local:8080/v1" class="bare">http://service.namespace.svc.cluster.local:8080/v1</a></code>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>model-name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model identifier for results</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>processor</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hugging Face model path for tokenizer</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>data-config</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Token configuration (<code>prompt_tokens=N,output_tokens=M</code>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>prompt_tokens=800,output_tokens=128</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>max-seconds</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Maximum benchmark duration in seconds</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>30</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>rate-type</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Benchmark rate type (<code>sweep</code>, <code>fixed</code>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>sweep</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>rate</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Request rate(s) for benchmarking</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>1.0, 4.0, 8.0, 16.0</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>api-key</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">OpenAI API key if required</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">``</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>max-concurrency</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Maximum concurrent requests</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>10</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>huggingface-token</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Hugging Face token for gated models</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">``</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>s3-api-endpoint</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">S3 endpoint URL (update to match your MinIO deployment)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code><a href="http://s3.openshift-storage.svc.cluster.local:80" class="bare">http://s3.openshift-storage.svc.cluster.local:80</a></code> (default) or <code><a href="http://minio-service.s3-storage.svc.cluster.local:9000" class="bare">http://minio-service.s3-storage.svc.cluster.local:9000</a></code> (MinIO)</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>s3-access-key-id</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">S3 access key ID</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>s3-secret-access-key</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">S3 secret access key</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>model-url</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ModelCar OCI image URL</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>lm-eval-job-name</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">LM-Eval job name to run</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>lm-eval-custom</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Whether to use custom evaluation (<code>True</code>/<code>False</code>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>False</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>custom-data</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Whether custom data is provided (<code>True</code>/<code>False</code>)</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>False</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>custom-filename</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Custom evaluation data filename</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>no-file</code></p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>model-reg-author</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Model registry author name</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock"><code>valuesContent</code></p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Helm values for model deployment</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">Required</p></td>
</tr>
</tbody>
</table>
</div>
</div>
<div class="sect1">
<h2 id="_building_custom_evaluation_containers"><a class="anchor" href="#_building_custom_evaluation_containers"></a>Building Custom Evaluation Containers</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The evaluation pipeline uses custom containers that can be built and customized:</p>
</div>
<div class="sect2">
<h3 id="_guidellm_evaluation_container"><a class="anchor" href="#_guidellm_evaluation_container"></a>GuideLLM Evaluation Container</h3>
<div class="paragraph">
<p>Location: <code>deploy/tasks/evaluate/</code></p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash">cd deploy/tasks/evaluate
chmod +x build-guidellm-container.sh

# Build with defaults
./build-guidellm-container.sh

# Or build manually
podman build --platform linux/amd64 \
  -t quay.io/&lt;YOUR_ORG&gt;/guidellm-ubi:latest \
  -f Containerfile .
podman push quay.io/&lt;YOUR_ORG&gt;/guidellm-ubi:latest</code></pre>
</div>
</div>
<div class="paragraph">
<p>Update the task YAML to reference your custom image:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml"># In guidellm-benchmark-task.yaml
steps:
  - name: guidellm-evaluate
    image: quay.io/&lt;YOUR_ORG&gt;/guidellm-ubi:latest</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_monitoring_evaluation_runs"><a class="anchor" href="#_monitoring_evaluation_runs"></a>Monitoring Evaluation Runs</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_view_pipeline_status"><a class="anchor" href="#_view_pipeline_status"></a>View Pipeline Status</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># List evaluation pipeline runs
oc get pipelinerun -l tekton.dev/pipeline=guidellm-benchmark-pipeline \
  -n modelcar-pipelines

# Watch specific run
oc get pipelinerun &lt;PIPELINERUN_NAME&gt; -n modelcar-pipelines -w

# Describe for details
oc describe pipelinerun &lt;PIPELINERUN_NAME&gt; -n modelcar-pipelines</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_view_task_logs"><a class="anchor" href="#_view_task_logs"></a>View Task Logs</h3>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># GuideLLM benchmark logs
oc logs -l tekton.dev/pipelineRun=&lt;PIPELINERUN_NAME&gt; \
  -l tekton.dev/pipelineTask=benchmark \
  -n modelcar-pipelines -f

# LM-Eval task logs (this task creates LMEvalJob in vllm namespace)
oc logs -l tekton.dev/pipelineRun=&lt;PIPELINERUN_NAME&gt; \
  -l tekton.dev/pipelineTask=lm-eval \
  -n modelcar-pipelines -f

# Check LMEvalJob pod logs in vllm namespace
oc logs -l job-name=&lt;LMEVAL_JOB_NAME&gt; -n vllm -f

# Model registry task logs
oc logs -l tekton.dev/pipelineRun=&lt;PIPELINERUN_NAME&gt; \
  -l tekton.dev/pipelineTask=register-model-and-results \
  -n modelcar-pipelines -f</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_view_results_in_s3"><a class="anchor" href="#_view_results_in_s3"></a>View Results in S3</h3>
<div class="paragraph">
<p>Access evaluation results in MinIO:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Get MinIO route
oc get route minio-api -n s3-storage

# Log in to MinIO console with credentials
# Navigate to buckets:
# - guidellm-results/ - GuideLLM benchmark results
# - lm-eval-results/ - LM-Eval task results</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_understanding_evaluation_results"><a class="anchor" href="#_understanding_evaluation_results"></a>Understanding Evaluation Results</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_guidellm_results"><a class="anchor" href="#_guidellm_results"></a>GuideLLM Results</h3>
<div class="paragraph">
<p>GuideLLM produces JSON files with performance metrics:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "model": "granite-3.3-2b-instruct",
  "metrics": {
    "throughput": {
      "requests_per_second": 15.2,
      "tokens_per_second": 1248.5
    },
    "latency": {
      "mean_ms": 125.3,
      "p50_ms": 110.2,
      "p95_ms": 180.5,
      "p99_ms": 250.1
    },
    "quality": {
      "time_to_first_token_ms": 45.2,
      "inter_token_latency_ms": 8.5
    }
  }
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Key metrics:
* <strong>Throughput</strong>: Requests and tokens per second
* <strong>Latency</strong>: Response time percentiles
* <strong>Quality</strong>: Time to first token, inter-token latency</p>
</div>
</div>
<div class="sect2">
<h3 id="_lm_eval_results"><a class="anchor" href="#_lm_eval_results"></a>LM-Eval Results</h3>
<div class="paragraph">
<p>LM-Eval produces accuracy scores for various tasks:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-json hljs" data-lang="json">{
  "results": {
    "mmlu": {
      "acc": 0.652,
      "acc_stderr": 0.012
    },
    "humaneval": {
      "pass@1": 0.347
    },
    "mbpp": {
      "pass@1": 0.412
    }
  },
  "config": {
    "model": "granite-3.3-2b-instruct",
    "tasks": ["mmlu", "humaneval", "mbpp"]
  }
}</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_creating_custom_evaluation_tasks"><a class="anchor" href="#_creating_custom_evaluation_tasks"></a>Creating Custom Evaluation Tasks</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_custom_lm_eval_task"><a class="anchor" href="#_custom_lm_eval_task"></a>Custom LM-Eval Task</h3>
<div class="paragraph">
<p>Create a custom LMEvalJob definition. The pipeline uses TrustyAI&#8217;s LMEvalJob Custom Resource:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: trustyai.opendatahub.io/v1alpha1
kind: LMEvalJob
metadata:
  name: custom-eval-job
  namespace: vllm
spec:
  model: local-completions  # or local-chat-completions for chat models
  taskList:
    taskNames:
      - custom-lmeval-task  # Your custom task name
  modelArgs:
    - name: model
      value: granite-2b  # Your InferenceService name
    - name: base_url
      value: http://granite-2b-predictor.vllm.svc.cluster.local:8080/v1/completions
    - name: include_path
      value: /opt/app-root/src/my_tasks  # Path where task.yaml is mounted
    - name: num_concurrent
      value: '1'
    - name: max_retries
      value: '6'
    - name: tokenized_requests
      value: 'False'
    - name: tokenizer
      value: ibm-granite/granite-3.3-8b-instruct
  logSamples: true
  batchSize: '1'
  allowOnline: true
  allowCodeExecution: false
  outputs:
    pvcManaged:
      size: 5Gi
  pod:
    volumes:
      - name: custom-benchmark-volume
        configMap:
          name: custom-lmeval-benchmark-files
    container:
      volumeMounts:
        - name: custom-benchmark-volume
          mountPath: /opt/app-root/src/my_tasks/task.yaml
          subPath: task.yaml
          readOnly: true
        - name: custom-benchmark-volume
          mountPath: /opt/app-root/src/my_tasks/data.jsonl
          subPath: data.jsonl
          readOnly: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>Create the ConfigMap with your custom task files:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Create ConfigMap with custom task definition and data
oc create configmap custom-lmeval-benchmark-files \
  --from-file=task.yaml=deploy/guidellm-pipeline/custom-lm-eval/task.yaml \
  --from-file=data.jsonl=deploy/guidellm-pipeline/custom-lm-eval/data.jsonl \
  -n vllm

# Update the custom-mmlu ConfigMap in pipeline namespace
oc create configmap custom-mmlu \
  --from-file=custom-mmlu.yaml=my-custom-eval-job.yaml \
  --from-file=task.yaml=deploy/guidellm-pipeline/custom-lm-eval/task.yaml \
  --from-file=custom-task.yaml=deploy/guidellm-pipeline/custom-lm-eval/custom-task.yaml \
  --from-file=data.jsonl=deploy/guidellm-pipeline/custom-lm-eval/data.jsonl \
  -n modelcar-pipelines \
  --dry-run=client -o yaml | oc apply -f -</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_custom_dataset"><a class="anchor" href="#_custom_dataset"></a>Custom Dataset</h3>
<div class="paragraph">
<p>Create a ConfigMap with custom evaluation data:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-yaml hljs" data-lang="yaml">apiVersion: v1
kind: ConfigMap
metadata:
  name: my-custom-dataset
  namespace: modelcar-pipelines
data:
  custom-data.json: |
    [
      {
        "input": "What is the capital of France?",
        "expected_output": "Paris"
      },
      {
        "input": "Explain quantum computing.",
        "expected_output": "Quantum computing uses quantum mechanics..."
      }
    ]</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_troubleshooting"><a class="anchor" href="#_troubleshooting"></a>Troubleshooting</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_model_endpoint_not_accessible"><a class="anchor" href="#_model_endpoint_not_accessible"></a>Model Endpoint Not Accessible</h3>
<div class="paragraph">
<p>Verify the InferenceService is running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Check InferenceService
oc get inferenceservice -n &lt;MODEL_NAMESPACE&gt;

# Check predictor pod
oc get pods -l serving.kserve.io/inferenceservice=&lt;MODEL_NAME&gt; \
  -n &lt;MODEL_NAMESPACE&gt;

# Test endpoint
oc run curl-test --rm -it --image=curlimages/curl -- \
  curl http://&lt;MODEL_SERVICE&gt;.&lt;NAMESPACE&gt;.svc.cluster.local:8080/v1/models</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_guidellm_benchmark_fails"><a class="anchor" href="#_guidellm_benchmark_fails"></a>GuideLLM Benchmark Fails</h3>
<div class="paragraph">
<p>Check GuideLLM task logs for errors:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># View detailed logs
oc logs -l tekton.dev/pipelineTask=benchmark \
  -l tekton.dev/pipelineRun=&lt;PIPELINERUN_NAME&gt; \
  -n modelcar-pipelines

# Common issues:
# - Model endpoint unreachable
# - Invalid data-config format
# - Timeout too short for large models</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_s3_upload_fails"><a class="anchor" href="#_s3_upload_fails"></a>S3 Upload Fails</h3>
<div class="paragraph">
<p>Verify MinIO credentials and connectivity:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># Check MinIO service
oc get svc minio-service -n s3-storage

# Test S3 access
oc run aws-cli --rm -it --image=amazon/aws-cli -- \
  s3 ls --endpoint-url http://minio-service.s3-storage.svc.cluster.local:9000 \
  --no-verify-ssl</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="_lm_eval_job_fails"><a class="anchor" href="#_lm_eval_job_fails"></a>LM-Eval Job Fails</h3>
<div class="paragraph">
<p>Check the LMEvalJob status and logs:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlightjs highlight"><code class="language-bash hljs" data-lang="bash"># List LMEvalJobs (in vllm namespace, not pipeline namespace)
oc get lmevaljob -n vllm

# Check LMEvalJob status
oc describe lmevaljob &lt;JOB_NAME&gt; -n vllm

# Check the pod created by LMEvalJob
oc get pods -n vllm | grep &lt;JOB_NAME&gt;

# Check pod logs
oc logs &lt;POD_NAME&gt; -n vllm

# Common issues:
# - Missing Hugging Face token for gated models (set in LMEvalJob spec)
# - Insufficient memory for large models
# - Invalid task configuration in ConfigMap
# - Model endpoint not accessible from vllm namespace
# - Missing custom task files in ConfigMap</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_best_practices"><a class="anchor" href="#_best_practices"></a>Best Practices</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_resource_allocation"><a class="anchor" href="#_resource_allocation"></a>Resource Allocation</h3>
<div class="ulist">
<ul>
<li>
<p>Allocate sufficient memory for evaluation tasks (8Gi+ for larger models)</p>
</li>
<li>
<p>Set appropriate timeouts based on model size and evaluation complexity</p>
</li>
<li>
<p>Use node selectors for GPU-enabled nodes if evaluating large models</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_result_management"><a class="anchor" href="#_result_management"></a>Result Management</h3>
<div class="ulist">
<ul>
<li>
<p>Regularly clean up old evaluation results from S3</p>
</li>
<li>
<p>Use versioning in model registry to track evaluation history</p>
</li>
<li>
<p>Archive important evaluation runs for compliance</p>
</li>
</ul>
</div>
</div>
<div class="sect2">
<h3 id="_performance_optimization"><a class="anchor" href="#_performance_optimization"></a>Performance Optimization</h3>
<div class="ulist">
<ul>
<li>
<p>Run multiple evaluation tasks in parallel when possible</p>
</li>
<li>
<p>Use shorter evaluation runs during development</p>
</li>
<li>
<p>Run comprehensive evaluations during production releases</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_next_steps"><a class="anchor" href="#_next_steps"></a>Next Steps</h2>
<div class="sectionbody">
<div class="ulist">
<ul>
<li>
<p>Integrate evaluation pipeline with CI/CD workflows</p>
</li>
<li>
<p>Create custom evaluation tasks for domain-specific testing</p>
</li>
<li>
<p>Set up automated alerts based on evaluation metrics</p>
</li>
<li>
<p>Build dashboards for evaluation result visualization</p>
</li>
</ul>
</div>
</div>
</div>
<nav class="pagination">
  <span class="prev"><a href="benchmark-pipeline.html">Benchmark Pipeline Deployment</a></span>
  <span class="next"><a href="results-analysis.html">Results Analysis and Visualization</a></span>
</nav>
</article>
  </div>
</main>
</div>
<footer class="footer">
  <img src="../../../_/img/rhl-logo-red.png" height="40px" alt="Red Hat"  href="https://redhat.com" >
</footer><script id="site-script" src="../../../_/js/site.js" data-ui-root-path="../../../_"></script>
<script async src="../../../_/js/vendor/highlight.js"></script>
  </body>
</html>
