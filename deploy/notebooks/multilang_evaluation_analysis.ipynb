{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Language LLM Evaluation Analysis\n",
    "\n",
    "This notebook analyzes and visualizes LLM performance across multiple languages.\n",
    "\n",
    "**Developed by**: Red Hat AI Customer Adoption and Innovation team (CAI)\n",
    "\n",
    "## Features\n",
    "- Load evaluation results from local storage or S3\n",
    "- Compare model performance across languages (English, Spanish, Japanese)\n",
    "- Visualize performance metrics with interactive charts\n",
    "- Generate comprehensive evaluation reports\n",
    "- Track performance over multiple evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (if not already installed)\n",
    "!pip install -q pandas matplotlib seaborn plotly boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration\n",
    "\n",
    "Configure where to load evaluation results from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== CONFIGURATION ==========\n",
    "\n",
    "# Data source: 'local' or 's3'\n",
    "DATA_SOURCE = 'local'\n",
    "\n",
    "# Local results directory\n",
    "LOCAL_RESULTS_DIR = Path(\"/workspace/shared-workspace/multilang_evaluation_results\")\n",
    "\n",
    "# S3 Configuration (if using S3)\n",
    "S3_CONFIG = {\n",
    "    'bucket': os.environ.get('S3_BUCKET', 'llm-evaluation-results'),\n",
    "    'endpoint_url': os.environ.get('S3_ENDPOINT_URL'),\n",
    "    'access_key': os.environ.get('AWS_ACCESS_KEY_ID'),\n",
    "    'secret_key': os.environ.get('AWS_SECRET_ACCESS_KEY'),\n",
    "}\n",
    "\n",
    "# Languages to analyze\n",
    "LANGUAGES = ['en', 'es', 'ja']\n",
    "LANGUAGE_NAMES = {\n",
    "    'en': 'English',\n",
    "    'es': 'Spanish (Espa√±ol)',\n",
    "    'ja': 'Japanese (Êó•Êú¨Ë™û)'\n",
    "}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Data Source: {DATA_SOURCE}\")\n",
    "if DATA_SOURCE == 'local':\n",
    "    print(f\"  Local Directory: {LOCAL_RESULTS_DIR}\")\n",
    "else:\n",
    "    print(f\"  S3 Bucket: {S3_CONFIG['bucket']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_local_results(results_dir: Path) -> Dict:\n",
    "    \"\"\"\n",
    "    Load evaluation results from local directory\n",
    "    \"\"\"\n",
    "    print(f\"üìÇ Loading results from: {results_dir}\")\n",
    "    \n",
    "    if not results_dir.exists():\n",
    "        raise FileNotFoundError(f\"Results directory not found: {results_dir}\")\n",
    "    \n",
    "    results = {\n",
    "        'summary': None,\n",
    "        'languages': {}\n",
    "    }\n",
    "    \n",
    "    # Load cross-language summary\n",
    "    summary_file = results_dir / \"cross_language_summary.json\"\n",
    "    if summary_file.exists():\n",
    "        with open(summary_file) as f:\n",
    "            results['summary'] = json.load(f)\n",
    "        print(f\"  ‚úì Loaded summary\")\n",
    "    \n",
    "    # Load language-specific results\n",
    "    for lang in LANGUAGES:\n",
    "        lang_dir = results_dir / lang\n",
    "        results_file = lang_dir / \"results.json\"\n",
    "        \n",
    "        if results_file.exists():\n",
    "            with open(results_file) as f:\n",
    "                results['languages'][lang] = json.load(f)\n",
    "            print(f\"  ‚úì Loaded {LANGUAGE_NAMES[lang]} results\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def load_s3_results(s3_prefix: str, config: Dict) -> Dict:\n",
    "    \"\"\"\n",
    "    Load evaluation results from S3\n",
    "    \"\"\"\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError\n",
    "    \n",
    "    print(f\"‚òÅÔ∏è  Loading results from S3: {s3_prefix}\")\n",
    "    \n",
    "    # Initialize S3 client\n",
    "    s3_client_config = {'service_name': 's3'}\n",
    "    if config.get('endpoint_url'):\n",
    "        s3_client_config['endpoint_url'] = config['endpoint_url']\n",
    "    if config.get('access_key') and config.get('secret_key'):\n",
    "        s3_client_config['aws_access_key_id'] = config['access_key']\n",
    "        s3_client_config['aws_secret_access_key'] = config['secret_key']\n",
    "    \n",
    "    s3 = boto3.client(**s3_client_config)\n",
    "    \n",
    "    results = {\n",
    "        'summary': None,\n",
    "        'languages': {}\n",
    "    }\n",
    "    \n",
    "    # Load cross-language summary\n",
    "    try:\n",
    "        response = s3.get_object(\n",
    "            Bucket=config['bucket'],\n",
    "            Key=f\"{s3_prefix}/cross_language_summary.json\"\n",
    "        )\n",
    "        results['summary'] = json.loads(response['Body'].read())\n",
    "        print(f\"  ‚úì Loaded summary\")\n",
    "    except ClientError:\n",
    "        print(f\"  ‚ö†Ô∏è  Summary not found\")\n",
    "    \n",
    "    # Load language-specific results\n",
    "    for lang in LANGUAGES:\n",
    "        try:\n",
    "            response = s3.get_object(\n",
    "                Bucket=config['bucket'],\n",
    "                Key=f\"{s3_prefix}/{lang}/results.json\"\n",
    "            )\n",
    "            results['languages'][lang] = json.loads(response['Body'].read())\n",
    "            print(f\"  ‚úì Loaded {LANGUAGE_NAMES[lang]} results\")\n",
    "        except ClientError:\n",
    "            print(f\"  ‚ö†Ô∏è  {LANGUAGE_NAMES[lang]} results not found\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "print(\"‚úÖ Data loading functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results based on configuration\n",
    "if DATA_SOURCE == 'local':\n",
    "    evaluation_results = load_local_results(LOCAL_RESULTS_DIR)\n",
    "else:\n",
    "    # Prompt for S3 prefix if not set\n",
    "    s3_prefix = input(\"Enter S3 prefix (e.g., evaluations/model_name/version/timestamp): \")\n",
    "    evaluation_results = load_s3_results(s3_prefix, S3_CONFIG)\n",
    "\n",
    "# Display summary\n",
    "if evaluation_results['summary']:\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"EVALUATION SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Model: {evaluation_results['summary'].get('model_name', 'Unknown')}\")\n",
    "    print(f\"Version: {evaluation_results['summary'].get('model_version', 'Unknown')}\")\n",
    "    print(f\"Timestamp: {evaluation_results['summary'].get('timestamp', 'Unknown')}\")\n",
    "    print(f\"Languages Evaluated: {', '.join(evaluation_results['languages'].keys())}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Processing and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metrics(results: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract metrics from results and create a pandas DataFrame\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for lang, lang_data in results['languages'].items():\n",
    "        for task, metrics in lang_data.get('results', {}).items():\n",
    "            if isinstance(metrics, dict):\n",
    "                for metric_name, value in metrics.items():\n",
    "                    if isinstance(value, (int, float)):\n",
    "                        data.append({\n",
    "                            'language': lang,\n",
    "                            'language_name': LANGUAGE_NAMES.get(lang, lang),\n",
    "                            'task': task,\n",
    "                            'metric': metric_name,\n",
    "                            'value': value\n",
    "                        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# Extract metrics\n",
    "df_metrics = extract_metrics(evaluation_results)\n",
    "\n",
    "print(f\"\\nüìä Extracted {len(df_metrics)} metric values\")\n",
    "print(f\"\\nSample data:\")\n",
    "display(df_metrics.head(10))\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nüìà Summary Statistics by Language:\")\n",
    "summary = df_metrics.groupby('language_name')['value'].agg(['mean', 'std', 'min', 'max', 'count'])\n",
    "display(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization: Performance Comparison Across Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for accuracy metrics only\n",
    "df_accuracy = df_metrics[\n",
    "    df_metrics['metric'].str.contains('acc|f1|em', case=False, na=False)\n",
    "]\n",
    "\n",
    "# Create bar chart comparing accuracy across languages\n",
    "fig = px.bar(\n",
    "    df_accuracy,\n",
    "    x='task',\n",
    "    y='value',\n",
    "    color='language_name',\n",
    "    barmode='group',\n",
    "    title='Model Performance Comparison Across Languages',\n",
    "    labels={'value': 'Score', 'task': 'Benchmark Task', 'language_name': 'Language'},\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pivot table for detailed view\n",
    "pivot_table = df_accuracy.pivot_table(\n",
    "    index=['task', 'metric'],\n",
    "    columns='language_name',\n",
    "    values='value',\n",
    "    aggfunc='first'\n",
    ").round(4)\n",
    "\n",
    "print(\"\\nüìã Detailed Performance Metrics:\")\n",
    "display(pivot_table)\n",
    "\n",
    "# Export to CSV\n",
    "output_file = Path(\"multilang_evaluation_summary.csv\")\n",
    "pivot_table.to_csv(output_file)\n",
    "print(f\"\\nüíæ Summary exported to: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Heatmap: Performance Across Tasks and Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate metrics by task and language\n",
    "heatmap_data = df_accuracy.groupby(['task', 'language_name'])['value'].mean().unstack()\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='RdYlGn',\n",
    "    cbar_kws={'label': 'Score'},\n",
    "    vmin=0,\n",
    "    vmax=1\n",
    ")\n",
    "plt.title('Performance Heatmap: Tasks vs Languages', fontsize=14, pad=20)\n",
    "plt.xlabel('Language', fontsize=12)\n",
    "plt.ylabel('Benchmark Task', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Language Performance Gap Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate performance gap relative to English\n",
    "if 'English' in heatmap_data.columns:\n",
    "    performance_gaps = pd.DataFrame()\n",
    "    \n",
    "    for lang in heatmap_data.columns:\n",
    "        if lang != 'English':\n",
    "            gaps = (heatmap_data[lang] - heatmap_data['English']) / heatmap_data['English'] * 100\n",
    "            performance_gaps[lang] = gaps\n",
    "    \n",
    "    # Plot performance gaps\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    performance_gaps.plot(kind='bar', ax=ax)\n",
    "    ax.axhline(y=0, color='black', linestyle='--', linewidth=0.5)\n",
    "    ax.set_title('Performance Gap Relative to English (%)', fontsize=14, pad=20)\n",
    "    ax.set_xlabel('Benchmark Task', fontsize=12)\n",
    "    ax.set_ylabel('Performance Gap (%)', fontsize=12)\n",
    "    ax.legend(title='Language', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüìä Average Performance Gap (vs English):\")\n",
    "    avg_gaps = performance_gaps.mean()\n",
    "    for lang, gap in avg_gaps.items():\n",
    "        print(f\"  {lang}: {gap:+.2f}%\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  English baseline not available for gap analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Comprehensive Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate markdown report\n",
    "report = []\n",
    "report.append(\"# Multi-Language LLM Evaluation Report\\n\")\n",
    "report.append(f\"**Generated**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "\n",
    "if evaluation_results['summary']:\n",
    "    summary = evaluation_results['summary']\n",
    "    report.append(\"## Model Information\\n\")\n",
    "    report.append(f\"- **Model**: {summary.get('model_name', 'Unknown')}\\n\")\n",
    "    report.append(f\"- **Version**: {summary.get('model_version', 'Unknown')}\\n\")\n",
    "    report.append(f\"- **Evaluation Date**: {summary.get('timestamp', 'Unknown')}\\n\")\n",
    "\n",
    "report.append(\"\\n## Performance Summary\\n\")\n",
    "report.append(f\"\\n{pivot_table.to_markdown()}\\n\")\n",
    "\n",
    "if 'English' in heatmap_data.columns and len(performance_gaps) > 0:\n",
    "    report.append(\"\\n## Performance Gap Analysis\\n\")\n",
    "    report.append(\"\\nAverage performance gap relative to English:\\n\")\n",
    "    for lang, gap in avg_gaps.items():\n",
    "        report.append(f\"- **{lang}**: {gap:+.2f}%\\n\")\n",
    "\n",
    "# Save report\n",
    "report_file = Path(\"multilang_evaluation_report.md\")\n",
    "with open(report_file, 'w') as f:\n",
    "    f.writelines(report)\n",
    "\n",
    "print(f\"\\n‚úÖ Comprehensive report saved to: {report_file}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Optional: Compare Multiple Evaluation Runs\n",
    "\n",
    "If you have multiple evaluation runs, you can load and compare them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Load multiple evaluation runs for comparison\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# evaluation_dirs = [\n",
    "#     Path(\"/workspace/shared-workspace/multilang_evaluation_results_v1\"),\n",
    "#     Path(\"/workspace/shared-workspace/multilang_evaluation_results_v2\"),\n",
    "# ]\n",
    "\n",
    "# all_results = {}\n",
    "# for eval_dir in evaluation_dirs:\n",
    "#     version = eval_dir.name.split('_')[-1]\n",
    "#     all_results[version] = load_local_results(eval_dir)\n",
    "\n",
    "# # Create comparison visualizations\n",
    "# # ... your comparison code here\n",
    "\n",
    "print(\"‚ÑπÔ∏è  To compare multiple runs, uncomment and configure the cell above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
